{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1118602d0>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from data_utils import load_CIFAR10\n",
    "import numpy as np\n",
    "import torch.utils.data as utils\n",
    "import time\n",
    "%matplotlib inline\n",
    "\n",
    "torch.manual_seed(1)    # reproducible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (49000, 3073)\n",
      "Train labels shape:  (49000,)\n",
      "Validation data shape:  (1000, 3073)\n",
      "Validation labels shape:  (1000,)\n",
      "Test data shape:  (1000, 3073)\n",
      "Test labels shape:  (1000,)\n",
      "dev data shape:  (500, 3073)\n",
      "dev labels shape:  (500,)\n"
     ]
    }
   ],
   "source": [
    "def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=1000, num_dev=500):\n",
    "    \"\"\"\n",
    "    Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n",
    "    it for the linear classifier. These are the same steps as we used for the\n",
    "    SVM, but condensed to a single function.  \n",
    "    \"\"\"\n",
    "    # Load the raw CIFAR-10 data\n",
    "    cifar10_dir = '../datasets/cifar-10-batches-py'\n",
    "    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "    \n",
    "    # subsample the data\n",
    "    mask = list(range(num_training, num_training + num_validation))\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = list(range(num_training))\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = list(range(num_test))\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "    mask = np.random.choice(num_training, num_dev, replace=False)\n",
    "    X_dev = X_train[mask]\n",
    "    y_dev = y_train[mask]\n",
    "    \n",
    "    # Preprocessing: reshape the image data into rows\n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
    "    X_val = np.reshape(X_val, (X_val.shape[0], -1))\n",
    "    X_test = np.reshape(X_test, (X_test.shape[0], -1))\n",
    "    X_dev = np.reshape(X_dev, (X_dev.shape[0], -1))\n",
    "    \n",
    "    # Normalize the data: subtract the mean image\n",
    "    mean_image = np.mean(X_train, axis = 0)\n",
    "    X_train -= mean_image\n",
    "    X_val -= mean_image\n",
    "    X_test -= mean_image\n",
    "    X_dev -= mean_image\n",
    "    \n",
    "    # add bias dimension and transform into columns\n",
    "    X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n",
    "    X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\n",
    "    X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n",
    "    X_dev = np.hstack([X_dev, np.ones((X_dev.shape[0], 1))])\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev\n",
    "\n",
    "\n",
    "# Invoke the above function to get our data.\n",
    "X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev = get_CIFAR10_data()\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)\n",
    "print('dev data shape: ', X_dev.shape)\n",
    "print('dev labels shape: ', y_dev.shape)\n",
    "\n",
    "X_train, y_train = torch.from_numpy(X_train).type(torch.FloatTensor), torch.from_numpy(y_train).type(torch.LongTensor)\n",
    "\n",
    "def variable_collate(batch):\n",
    "    \"\"\"Puts batch of inputs, labels each into a Variable.\n",
    "       Args:\n",
    "         batch: (list) [inputs, labels].  In this simple example, I'm just assuming the input and labels are already Tensor types\n",
    "       Output:\n",
    "         minibatch: (Variable)\n",
    "         targets: (Variable)\n",
    "    \"\"\"\n",
    "    minibatch, targets = zip(*[(a, b) for (a,b) in batch])\n",
    "    minibatch, targets = torch.stack(minibatch, dim=0), torch.stack(targets, dim=0)\n",
    "    minibatch, targets = Variable(minibatch), Variable(targets)\n",
    "    return minibatch, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, n_feature, n_hidden, n_output):\n",
    "        super(Net, self).__init__()\n",
    "        self.hidden = torch.nn.Linear(n_feature, n_hidden)   # hidden layer\n",
    "        self.out = torch.nn.Linear(n_hidden, n_output)   # output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.hidden(x))      # activation function for hidden layer\n",
    "        x = self.out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (hidden): Linear(in_features=3073, out_features=1024, bias=True)\n",
      "  (out): Linear(in_features=1024, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = Net(n_feature=3073, n_hidden=1024, n_output=10)     # define the network\n",
    "print(net)  # net architecture\n",
    "\n",
    "# Loss and Optimizer\n",
    "# Softmax is internally computed.\n",
    "# Set parameters to be updated.\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.02)\n",
    "loss_func = torch.nn.CrossEntropyLoss()  # the target label is NOT an one-hotted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.ion()   # something about plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/python3/3.6.4_2/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:40: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy=0.08\n",
      "Accuracy=0.16\n",
      "Accuracy=0.06\n",
      "Accuracy=0.14\n",
      "Accuracy=0.11\n",
      "Accuracy=0.08\n",
      "Accuracy=0.14\n",
      "Accuracy=0.08\n",
      "Accuracy=0.08\n",
      "Accuracy=0.03\n",
      "Accuracy=0.11\n",
      "Accuracy=0.05\n",
      "Accuracy=0.12\n",
      "Accuracy=0.09\n",
      "Accuracy=0.16\n",
      "Accuracy=0.11\n",
      "Accuracy=0.09\n",
      "Accuracy=0.08\n",
      "Accuracy=0.08\n",
      "Accuracy=0.19\n",
      "Accuracy=0.14\n",
      "Accuracy=0.06\n",
      "Accuracy=0.12\n",
      "Accuracy=0.11\n",
      "Accuracy=0.09\n",
      "Accuracy=0.08\n",
      "Accuracy=0.02\n",
      "Accuracy=0.06\n",
      "Accuracy=0.11\n",
      "Accuracy=0.14\n",
      "Accuracy=0.09\n",
      "Accuracy=0.12\n",
      "Accuracy=0.12\n",
      "Accuracy=0.09\n",
      "Accuracy=0.03\n",
      "Accuracy=0.09\n",
      "Accuracy=0.12\n",
      "Accuracy=0.11\n",
      "Accuracy=0.12\n",
      "Accuracy=0.05\n",
      "Accuracy=0.08\n",
      "Accuracy=0.05\n",
      "Accuracy=0.11\n",
      "Accuracy=0.03\n",
      "Accuracy=0.12\n",
      "Accuracy=0.12\n",
      "Accuracy=0.06\n",
      "Accuracy=0.17\n",
      "Accuracy=0.08\n",
      "Accuracy=0.12\n",
      "Accuracy=0.03\n",
      "Accuracy=0.05\n",
      "Accuracy=0.08\n",
      "Accuracy=0.11\n",
      "Accuracy=0.11\n",
      "Accuracy=0.05\n",
      "Accuracy=0.08\n",
      "Accuracy=0.11\n",
      "Accuracy=0.05\n",
      "Accuracy=0.11\n",
      "Accuracy=0.05\n",
      "Accuracy=0.08\n",
      "Accuracy=0.14\n",
      "Accuracy=0.06\n",
      "Accuracy=0.14\n",
      "Accuracy=0.08\n",
      "Accuracy=0.12\n",
      "Accuracy=0.11\n",
      "Accuracy=0.11\n",
      "Accuracy=0.05\n",
      "Accuracy=0.03\n",
      "Accuracy=0.17\n",
      "Accuracy=0.14\n",
      "Accuracy=0.17\n",
      "Accuracy=0.16\n",
      "Accuracy=0.09\n",
      "Accuracy=0.11\n",
      "Accuracy=0.06\n",
      "Accuracy=0.11\n",
      "Accuracy=0.16\n",
      "Accuracy=0.11\n",
      "Accuracy=0.09\n",
      "Accuracy=0.08\n",
      "Accuracy=0.05\n",
      "Accuracy=0.09\n",
      "Accuracy=0.11\n",
      "Accuracy=0.06\n",
      "Accuracy=0.08\n",
      "Accuracy=0.14\n",
      "Accuracy=0.09\n",
      "Accuracy=0.14\n",
      "Accuracy=0.08\n",
      "Accuracy=0.12\n",
      "Accuracy=0.11\n",
      "Accuracy=0.09\n",
      "Accuracy=0.09\n",
      "Accuracy=0.14\n",
      "Accuracy=0.11\n",
      "Accuracy=0.14\n",
      "Accuracy=0.08\n",
      "Accuracy=0.16\n",
      "Accuracy=0.20\n",
      "Accuracy=0.09\n",
      "Accuracy=0.11\n",
      "Accuracy=0.08\n",
      "Accuracy=0.17\n",
      "Accuracy=0.17\n",
      "Accuracy=0.03\n",
      "Accuracy=0.05\n",
      "Accuracy=0.11\n",
      "Accuracy=0.08\n",
      "Accuracy=0.09\n",
      "Accuracy=0.11\n",
      "Accuracy=0.06\n",
      "Accuracy=0.08\n",
      "Accuracy=0.06\n",
      "Accuracy=0.09\n",
      "Accuracy=0.12\n",
      "Accuracy=0.05\n",
      "Accuracy=0.08\n",
      "Accuracy=0.06\n",
      "Accuracy=0.14\n",
      "Accuracy=0.12\n",
      "Accuracy=0.05\n",
      "Accuracy=0.06\n",
      "Accuracy=0.08\n",
      "Accuracy=0.00\n",
      "Accuracy=0.05\n",
      "Accuracy=0.08\n",
      "Accuracy=0.06\n",
      "Accuracy=0.08\n",
      "Accuracy=0.12\n",
      "Accuracy=0.08\n",
      "Accuracy=0.11\n",
      "Accuracy=0.09\n",
      "Accuracy=0.06\n",
      "Accuracy=0.11\n",
      "Accuracy=0.12\n",
      "Accuracy=0.12\n",
      "Accuracy=0.16\n",
      "Accuracy=0.16\n",
      "Accuracy=0.09\n",
      "Accuracy=0.08\n",
      "Accuracy=0.16\n",
      "Accuracy=0.16\n",
      "Accuracy=0.12\n",
      "Accuracy=0.12\n",
      "Accuracy=0.02\n",
      "Accuracy=0.09\n",
      "Accuracy=0.08\n",
      "Accuracy=0.11\n",
      "Accuracy=0.09\n",
      "Accuracy=0.05\n",
      "Accuracy=0.11\n",
      "Accuracy=0.09\n",
      "Accuracy=0.11\n",
      "Accuracy=0.09\n",
      "Accuracy=0.06\n",
      "Accuracy=0.05\n",
      "Accuracy=0.17\n",
      "Accuracy=0.11\n",
      "Accuracy=0.19\n",
      "Accuracy=0.08\n",
      "Accuracy=0.06\n",
      "Accuracy=0.05\n",
      "Accuracy=0.11\n",
      "Accuracy=0.06\n",
      "Accuracy=0.12\n",
      "Accuracy=0.09\n",
      "Accuracy=0.03\n",
      "Accuracy=0.12\n",
      "Accuracy=0.14\n",
      "Accuracy=0.09\n",
      "Accuracy=0.06\n",
      "Accuracy=0.06\n",
      "Accuracy=0.08\n",
      "Accuracy=0.11\n",
      "Accuracy=0.09\n",
      "Accuracy=0.08\n",
      "Accuracy=0.08\n",
      "Accuracy=0.09\n",
      "Accuracy=0.12\n",
      "Accuracy=0.08\n",
      "Accuracy=0.06\n",
      "Accuracy=0.06\n",
      "Accuracy=0.08\n",
      "Accuracy=0.08\n",
      "Accuracy=0.11\n",
      "Accuracy=0.12\n",
      "Accuracy=0.11\n",
      "Accuracy=0.11\n",
      "Accuracy=0.14\n",
      "Accuracy=0.11\n",
      "Accuracy=0.16\n",
      "Accuracy=0.11\n",
      "Accuracy=0.08\n",
      "Accuracy=0.02\n",
      "Accuracy=0.05\n",
      "Accuracy=0.11\n",
      "Accuracy=0.08\n",
      "Accuracy=0.08\n",
      "Accuracy=0.06\n",
      "Accuracy=0.14\n",
      "Accuracy=0.08\n",
      "Accuracy=0.05\n",
      "Accuracy=0.11\n",
      "Accuracy=0.08\n",
      "Accuracy=0.09\n",
      "Accuracy=0.19\n",
      "Accuracy=0.09\n",
      "Accuracy=0.12\n",
      "Accuracy=0.16\n",
      "Accuracy=0.09\n",
      "Accuracy=0.12\n",
      "Accuracy=0.16\n",
      "Accuracy=0.08\n",
      "Accuracy=0.08\n",
      "Accuracy=0.11\n",
      "Accuracy=0.09\n",
      "Accuracy=0.14\n",
      "Accuracy=0.08\n",
      "Accuracy=0.20\n",
      "Accuracy=0.11\n",
      "Accuracy=0.14\n",
      "Accuracy=0.11\n",
      "Accuracy=0.09\n",
      "Accuracy=0.12\n",
      "Accuracy=0.09\n",
      "Accuracy=0.12\n",
      "Accuracy=0.09\n",
      "Accuracy=0.11\n",
      "Accuracy=0.06\n",
      "Accuracy=0.06\n",
      "Accuracy=0.14\n",
      "Accuracy=0.11\n",
      "Accuracy=0.11\n",
      "Accuracy=0.16\n",
      "Accuracy=0.14\n",
      "Accuracy=0.08\n",
      "Accuracy=0.08\n",
      "Accuracy=0.12\n",
      "Accuracy=0.06\n",
      "Accuracy=0.09\n",
      "Accuracy=0.11\n",
      "Accuracy=0.08\n",
      "Accuracy=0.16\n",
      "Accuracy=0.08\n",
      "Accuracy=0.09\n",
      "Accuracy=0.14\n",
      "Accuracy=0.08\n",
      "Accuracy=0.14\n",
      "Accuracy=0.11\n",
      "Accuracy=0.08\n",
      "Accuracy=0.08\n",
      "Accuracy=0.17\n",
      "Accuracy=0.14\n",
      "Accuracy=0.16\n",
      "Accuracy=0.12\n",
      "Accuracy=0.06\n",
      "Accuracy=0.11\n",
      "Accuracy=0.08\n",
      "Accuracy=0.05\n",
      "Accuracy=0.11\n",
      "Accuracy=0.09\n",
      "Accuracy=0.06\n",
      "Accuracy=0.09\n",
      "Accuracy=0.16\n",
      "Accuracy=0.14\n",
      "Accuracy=0.08\n",
      "Accuracy=0.09\n",
      "Accuracy=0.05\n",
      "Accuracy=0.12\n",
      "Accuracy=0.11\n",
      "Accuracy=0.12\n",
      "Accuracy=0.12\n",
      "Accuracy=0.11\n",
      "Accuracy=0.02\n",
      "Accuracy=0.17\n",
      "Accuracy=0.06\n",
      "Accuracy=0.11\n",
      "Accuracy=0.11\n",
      "Accuracy=0.12\n",
      "Accuracy=0.08\n",
      "Accuracy=0.06\n",
      "Accuracy=0.08\n",
      "Accuracy=0.11\n",
      "Accuracy=0.09\n",
      "Accuracy=0.14\n",
      "Accuracy=0.08\n",
      "Accuracy=0.08\n",
      "Accuracy=0.08\n",
      "Accuracy=0.08\n",
      "Accuracy=0.08\n",
      "Accuracy=0.14\n",
      "Accuracy=0.11\n",
      "Accuracy=0.12\n",
      "Accuracy=0.14\n",
      "Accuracy=0.06\n",
      "Accuracy=0.11\n",
      "Accuracy=0.11\n",
      "Accuracy=0.09\n",
      "Accuracy=0.09\n",
      "Accuracy=0.08\n",
      "Accuracy=0.06\n",
      "Accuracy=0.11\n",
      "Accuracy=0.08\n",
      "Accuracy=0.05\n",
      "Accuracy=0.16\n",
      "Accuracy=0.11\n",
      "Accuracy=0.06\n",
      "Accuracy=0.19\n",
      "Accuracy=0.19\n",
      "Accuracy=0.11\n",
      "Accuracy=0.06\n",
      "Accuracy=0.08\n",
      "Accuracy=0.11\n",
      "Accuracy=0.11\n",
      "Accuracy=0.09\n",
      "Accuracy=0.16\n",
      "Accuracy=0.06\n",
      "Accuracy=0.05\n",
      "Accuracy=0.08\n",
      "Accuracy=0.06\n",
      "Accuracy=0.11\n",
      "Accuracy=0.11\n",
      "Accuracy=0.11\n",
      "Accuracy=0.09\n",
      "Accuracy=0.12\n",
      "Accuracy=0.08\n",
      "Accuracy=0.12\n",
      "Accuracy=0.11\n",
      "Accuracy=0.06\n",
      "Accuracy=0.05\n",
      "Accuracy=0.03\n",
      "Accuracy=0.09\n",
      "Accuracy=0.06\n",
      "Accuracy=0.11\n",
      "Accuracy=0.08\n",
      "Accuracy=0.12\n",
      "Accuracy=0.11\n",
      "Accuracy=0.16\n",
      "Accuracy=0.06\n",
      "Accuracy=0.11\n",
      "Accuracy=0.11\n",
      "Accuracy=0.16\n",
      "Accuracy=0.11\n",
      "Accuracy=0.09\n",
      "Accuracy=0.06\n",
      "Accuracy=0.11\n",
      "Accuracy=0.11\n",
      "Accuracy=0.16\n",
      "Accuracy=0.06\n",
      "Accuracy=0.05\n",
      "Accuracy=0.20\n",
      "Accuracy=0.06\n",
      "Accuracy=0.11\n",
      "Accuracy=0.09\n",
      "Accuracy=0.14\n",
      "Accuracy=0.08\n",
      "Accuracy=0.09\n",
      "Accuracy=0.12\n",
      "Accuracy=0.12\n",
      "Accuracy=0.09\n",
      "Accuracy=0.05\n",
      "Accuracy=0.16\n",
      "Accuracy=0.11\n",
      "Accuracy=0.08\n",
      "Accuracy=0.08\n",
      "Accuracy=0.11\n",
      "Accuracy=0.06\n",
      "Accuracy=0.12\n",
      "Accuracy=0.11\n",
      "Accuracy=0.17\n",
      "Accuracy=0.09\n",
      "Accuracy=0.08\n",
      "Accuracy=0.08\n",
      "Accuracy=0.09\n",
      "Accuracy=0.09\n",
      "Accuracy=0.08\n",
      "Accuracy=0.11\n",
      "Accuracy=0.08\n",
      "Accuracy=0.09\n",
      "Accuracy=0.05\n",
      "7660\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "for t in range(100):\n",
    "    out = net(X_train)                 # input x and predict based on x\n",
    "    loss = loss_func(out, y_train)     # must be (1. nn output, 2. target), the target label is NOT one-hotted\n",
    "\n",
    "    optimizer.zero_grad()   # clear gradients for next train\n",
    "    loss.backward()         # backpropagation, compute gradients\n",
    "    optimizer.step()        # apply gradients\n",
    "    \n",
    "    if t % 10 == 0 or t in [3, 6]:\n",
    "        # show learning process\n",
    "        _, prediction = torch.max(F.softmax(out), 1)\n",
    "        pred_y = prediction.data.numpy().squeeze()\n",
    "        target_y = y_train.data.numpy()\n",
    "        accuracy = sum(pred_y == target_y)/200.\n",
    "        print('Accuracy=%.2f' % accuracy)\n",
    "'''\n",
    "traindataset = utils.TensorDataset(X_train, y_train)\n",
    "trainloader = utils.DataLoader(traindataset, batch_size=64, shuffle=True)\n",
    "#trainloader.batch_size = 128\n",
    "\n",
    "epochs = 10\n",
    "steps = 0\n",
    "print_every = 20\n",
    "for e in range(epochs):\n",
    "    start = time.time()\n",
    "    for images, labels in iter(trainloader):\n",
    "        images, labels = Variable(images), Variable(labels)\n",
    "        steps += 1\n",
    "        \n",
    "        out = net.forward(images)\n",
    "        loss = loss_func(out, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if steps % print_every == 0:\n",
    "            stop = time.time()\n",
    "            # Test accuracy\n",
    "            _, prediction = torch.max(F.softmax(out), 1)\n",
    "            pred_y = prediction.data.numpy().squeeze()\n",
    "            target_y = labels.data.numpy()\n",
    "            accuracy = sum(pred_y == target_y)/64.\n",
    "            print('Accuracy=%.2f' % accuracy)\n",
    "            \n",
    "            start = time.time()\n",
    "print(steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
