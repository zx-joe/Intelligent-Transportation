{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Python: Exercise 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression with one variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first part of the exercise, we're tasked with implementing linear regression with one variable to predict profits for a food truck. Suppose you are the CEO of a restaurant franchise and are considering different cities for opening a new outlet. The chain already has trucks in various cities and you have data for profits and populations from the cities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by importing some libraries and examining the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the data from the CSV file using Panda library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Population</th>\n",
       "      <th>Profit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.1101</td>\n",
       "      <td>17.5920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.5277</td>\n",
       "      <td>9.1302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.5186</td>\n",
       "      <td>13.6620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.0032</td>\n",
       "      <td>11.8540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.8598</td>\n",
       "      <td>6.8233</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Population   Profit\n",
       "0      6.1101  17.5920\n",
       "1      5.5277   9.1302\n",
       "2      8.5186  13.6620\n",
       "3      7.0032  11.8540\n",
       "4      5.8598   6.8233"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path =  'ex1data1.txt'\n",
    "data = pd.read_csv(path, header=None, names=['Population', 'Profit'])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Population</th>\n",
       "      <th>Profit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>97.000000</td>\n",
       "      <td>97.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>8.159800</td>\n",
       "      <td>5.839135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.869884</td>\n",
       "      <td>5.510262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>5.026900</td>\n",
       "      <td>-2.680700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>5.707700</td>\n",
       "      <td>1.986900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>6.589400</td>\n",
       "      <td>4.562300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>8.578100</td>\n",
       "      <td>7.046700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>22.203000</td>\n",
       "      <td>24.147000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Population     Profit\n",
       "count   97.000000  97.000000\n",
       "mean     8.159800   5.839135\n",
       "std      3.869884   5.510262\n",
       "min      5.026900  -2.680700\n",
       "25%      5.707700   1.986900\n",
       "50%      6.589400   4.562300\n",
       "75%      8.578100   7.046700\n",
       "max     22.203000  24.147000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot it to get a better idea of what the data looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1b179d2e710>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data.plot(kind='scatter', x='Population', y='Profit', figsize=(12,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's implement linear regression using gradient descent to minimize the cost function.  The equations implemented in the following code samples are detailed in \"ex1.pdf\" in the \"exercises\" folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll create a function to compute the cost of a given solution (characterized by the parameters theta). The cost function is the Mean Sqaured error in matrix form: \n",
    "\n",
    "$$ MSE(\\theta) = \\frac{1}{N}\\sum_n^N [ y_n-x_n^T*\\theta]^2 $$\n",
    "\n",
    "where $\\theta$ and $x_n$ are vectors\n",
    "\n",
    "__Hint__: Use the matrix form of the cost function and make use of numpy operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeCost(x, y, theta):\n",
    "    e = y - x.dot(theta)\n",
    "    return np.mean(np.square(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add a column of ones to the training set so we can use a vectorized solution to computing the cost and gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.insert(0, 'Ones', 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's do some variable initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set X (training data) and y (target variable)\n",
    "cols = data.shape[1]\n",
    "X = data.iloc[:,0:cols-1]\n",
    "y = data.iloc[:,cols-1:cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look to make sure X (training set) and y (target variable) look correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ones</th>\n",
       "      <th>Population</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>6.1101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>5.5277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>8.5186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>7.0032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5.8598</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Ones  Population\n",
       "0     1      6.1101\n",
       "1     1      5.5277\n",
       "2     1      8.5186\n",
       "3     1      7.0032\n",
       "4     1      5.8598"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Profit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.5920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9.1302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.6620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.8540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.8233</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Profit\n",
       "0  17.5920\n",
       "1   9.1302\n",
       "2  13.6620\n",
       "3  11.8540\n",
       "4   6.8233"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert X and Y to numpy array for better manipulation. Initiliaze Theta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array(X.values)\n",
    "y = np.array(y.values).flatten()\n",
    "theta = np.array([0,0])\n",
    "theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a quick look at the shape of our matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((97, 2), (2,), (97,))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, theta.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compute the cost for our initial solution (0 values for theta)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64.14546775491135"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "computeCost(X, y, theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far so good.  Now we need to define a function to perform gradient descent on the parameters theta using the update rules. Write first a function that computes the gradient of a matrix and then use it in the gradientDescent function.\n",
    "\n",
    "The gradient descent formula is:\n",
    "\n",
    "$$\\theta^{t+1} = \\theta^{t} - \\alpha*\\nabla MSE(\\theta^{t})$$\n",
    "\n",
    "where $\\nabla MSE(\\theta^{t})$ is the gradient of the cost function at $\\theta^{t}$\n",
    "\n",
    "__Hint__: Use the matrix form of the gradient and make use of numpy operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(y, tx, w):\n",
    "    \"\"\"Compute the gradient.\"\"\"\n",
    "    err = y - tx.dot(w)\n",
    "    grad = -tx.T.dot(err) / len(err)\n",
    "    return grad, err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientDescent(X, y, theta, alpha,max_iters):\n",
    "    \"\"\"Gradient descent algorithm.\"\"\"\n",
    "    ws = [theta]\n",
    "    cost = np.zeros(max_iters)\n",
    "    for n_iter in range(max_iters):\n",
    "        grad, err = compute_gradient(y, X, theta)\n",
    "        loss = 1/2*np.mean(err**2)\n",
    "        theta= theta - alpha * grad\n",
    "        ws.append(theta)\n",
    "        cost[n_iter] = loss\n",
    "        print(\"Gradient Descent({bi}/{ti}): loss={l}, w0={w0}, w1={w1}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss, w0=theta[0], w1=theta[1]))\n",
    "\n",
    "    return theta, cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize some additional variables - the learning rate alpha, and the number of iterations to perform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.01\n",
    "iters = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run the gradient descent algorithm to fit our parameters theta to the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/999): loss=32.072733877455676, w0=0.05839135051546391, w1=0.6532884974555669\n",
      "Gradient Descent(1/999): loss=6.737190464870009, w0=0.06289175271039384, w1=0.7700097825599364\n",
      "Gradient Descent(2/999): loss=5.931593568604956, w0=0.05782292746142813, w1=0.7913481156584673\n",
      "Gradient Descent(3/999): loss=5.901154707081388, w0=0.05106362516077815, w1=0.795729810284954\n",
      "Gradient Descent(4/999): loss=5.895228586444222, w0=0.044014378365002604, w1=0.7970961782721866\n",
      "Gradient Descent(5/999): loss=5.89009494311733, w0=0.036924131142162614, w1=0.7979254732843951\n",
      "Gradient Descent(6/999): loss=5.885004158443646, w0=0.029837117577144835, w1=0.7986582394519285\n",
      "Gradient Descent(7/999): loss=5.8799324804914175, w0=0.02276118189403884, w1=0.7993727912003019\n",
      "Gradient Descent(8/999): loss=5.874879094762575, w0=0.015697699574200134, w1=0.8000830518518655\n",
      "Gradient Descent(9/999): loss=5.869843911806388, w0=0.008646896228913532, w1=0.8007914983590768\n",
      "Gradient Descent(10/999): loss=5.8648268653129305, w0=0.001608793098984364, w1=0.8014985729280016\n",
      "Gradient Descent(11/999): loss=5.859827889932181, w0=-0.005416624870320639, w1=0.8022043560583255\n",
      "Gradient Descent(12/999): loss=5.85484692057229, w0=-0.01242937915180076, w1=0.8029088639482521\n",
      "Gradient Descent(13/999): loss=5.849883892376587, w0=-0.019429492325268312, w1=0.8036121013621462\n",
      "Gradient Descent(14/999): loss=5.844938740722034, w0=-0.026416987133500117, w1=0.8043140710284592\n",
      "Gradient Descent(15/999): loss=5.840011401218361, w0=-0.03339188631448141, w1=0.8050147753103406\n",
      "Gradient Descent(16/999): loss=5.8351018097072265, w0=-0.04035421257164585, w1=0.8057142165026173\n",
      "Gradient Descent(17/999): loss=5.830209902261388, w0=-0.04730398856864604, w1=0.8064123968845912\n",
      "Gradient Descent(18/999): loss=5.825335615183862, w0=-0.05424123692848453, w1=0.8071093187294315\n",
      "Gradient Descent(19/999): loss=5.820478885007099, w0=-0.06116598023341992, w1=0.8078049843058496\n",
      "Gradient Descent(20/999): loss=5.815639648492154, w0=-0.06807824102501052, w1=0.8084993958784037\n",
      "Gradient Descent(21/999): loss=5.81081784262787, w0=-0.07497804180418248, w1=0.8091925557075582\n",
      "Gradient Descent(22/999): loss=5.806013404630044, w0=-0.08186540503130207, w1=0.8098844660497012\n",
      "Gradient Descent(23/999): loss=5.80122627194063, w0=-0.08874035312624866, w1=0.8105751291571527\n",
      "Gradient Descent(24/999): loss=5.7964563822269, w0=-0.0956029084684876, w1=0.8112645472781728\n",
      "Gradient Descent(25/999): loss=5.791703673380653, w0=-0.10245309339714315, w1=0.8119527226569687\n",
      "Gradient Descent(26/999): loss=5.786968083517396, w0=-0.10929093021107113, w1=0.8126396575337025\n",
      "Gradient Descent(27/999): loss=5.782249550975539, w0=-0.11611644116893155, w1=0.813325354144498\n",
      "Gradient Descent(28/999): loss=5.777548014315596, w0=-0.12292964848926106, w1=0.8140098147214483\n",
      "Gradient Descent(29/999): loss=5.772863412319381, w0=-0.12973057435054527, w1=0.8146930414926226\n",
      "Gradient Descent(30/999): loss=5.768195683989214, w0=-0.13651924089129092, w1=0.8153750366820742\n",
      "Gradient Descent(31/999): loss=5.76354476854712, w0=-0.14329567021009798, w1=0.8160558025098472\n",
      "Gradient Descent(32/999): loss=5.758910605434049, w0=-0.1500598843657316, w1=0.8167353411919838\n",
      "Gradient Descent(33/999): loss=5.754293134309077, w0=-0.15681190537719386, w1=0.8174136549405314\n",
      "Gradient Descent(34/999): loss=5.749692295048629, w0=-0.16355175522379548, w1=0.8180907459635505\n",
      "Gradient Descent(35/999): loss=5.745108027745684, w0=-0.17027945584522738, w1=0.8187666164651208\n",
      "Gradient Descent(36/999): loss=5.740540272709012, w0=-0.17699502914163212, w1=0.8194412686453493\n",
      "Gradient Descent(37/999): loss=5.735988970462381, w0=-0.1836984969736751, w1=0.8201147047003767\n",
      "Gradient Descent(38/999): loss=5.731454061743792, w0=-0.19038988116261576, w1=0.8207869268223856\n",
      "Gradient Descent(39/999): loss=5.7269354875047, w0=-0.1970692034903787, w1=0.8214579371996062\n",
      "Gradient Descent(40/999): loss=5.722433188909255, w0=-0.20373648569962446, w1=0.822127738016325\n",
      "Gradient Descent(41/999): loss=5.717947107333528, w0=-0.2103917494938204, w1=0.8227963314528903\n",
      "Gradient Descent(42/999): loss=5.71347718436475, w0=-0.21703501653731122, w1=0.8234637196857207\n",
      "Gradient Descent(43/999): loss=5.709023361800548, w0=-0.22366630845538962, w1=0.8241299048873114\n",
      "Gradient Descent(44/999): loss=5.704585581648199, w0=-0.23028564683436664, w1=0.8247948892262416\n",
      "Gradient Descent(45/999): loss=5.700163786123856, w0=-0.23689305322164192, w1=0.8254586748671812\n",
      "Gradient Descent(46/999): loss=5.695757917651815, w0=-0.24348854912577383, w1=0.8261212639708981\n",
      "Gradient Descent(47/999): loss=5.691367918863752, w0=-0.25007215601654953, w1=0.8267826586942654\n",
      "Gradient Descent(48/999): loss=5.68699373259798, w0=-0.25664389532505477, w1=0.8274428611902682\n",
      "Gradient Descent(49/999): loss=5.682635301898707, w0=-0.2632037884437438, w1=0.8281018736080105\n",
      "Gradient Descent(50/999): loss=5.678292570015292, w0=-0.2697518567265089, w1=0.8287596980927223\n",
      "Gradient Descent(51/999): loss=5.673965480401505, w0=-0.27628812148874987, w1=0.8294163367857669\n",
      "Gradient Descent(52/999): loss=5.669653976714797, w0=-0.28281260400744346, w1=0.8300717918246473\n",
      "Gradient Descent(53/999): loss=5.6653580028155535, w0=-0.28932532552121265, w1=0.8307260653430135\n",
      "Gradient Descent(54/999): loss=5.661077502766379, w0=-0.2958263072303958, w1=0.8313791594706695\n",
      "Gradient Descent(55/999): loss=5.6568124208313595, w0=-0.30231557029711564, w1=0.8320310763335801\n",
      "Gradient Descent(56/999): loss=5.6525627014753335, w0=-0.308793135845348, w1=0.8326818180538778\n",
      "Gradient Descent(57/999): loss=5.648328289363181, w0=-0.31525902496099095, w1=0.8333313867498697\n",
      "Gradient Descent(58/999): loss=5.644109129359092, w0=-0.321713258691933, w1=0.8339797845360446\n",
      "Gradient Descent(59/999): loss=5.639905166525856, w0=-0.3281558580481219, w1=0.8346270135230796\n",
      "Gradient Descent(60/999): loss=5.635716346124135, w0=-0.334586844001633, w1=0.835273075817847\n",
      "Gradient Descent(61/999): loss=5.631542613611772, w0=-0.3410062374867375, w1=0.8359179735234217\n",
      "Gradient Descent(62/999): loss=5.627383914643055, w0=-0.34741405939997033, w1=0.8365617087390872\n",
      "Gradient Descent(63/999): loss=5.623240195068026, w0=-0.35381033060019873, w1=0.8372042835603428\n",
      "Gradient Descent(64/999): loss=5.619111400931778, w0=-0.36019507190868966, w1=0.8378457000789108\n",
      "Gradient Descent(65/999): loss=5.61499747847374, w0=-0.3665683041091778, w1=0.8384859603827427\n",
      "Gradient Descent(66/999): loss=5.610898374126984, w0=-0.3729300479479331, w1=0.8391250665560263\n",
      "Gradient Descent(67/999): loss=5.606814034517532, w0=-0.3792803241338285, w1=0.8397630206791926\n",
      "Gradient Descent(68/999): loss=5.602744406463648, w0=-0.3856191533384071, w1=0.8403998248289224\n",
      "Gradient Descent(69/999): loss=5.598689436975159, w0=-0.3919465561959495, w1=0.8410354810781527\n",
      "Gradient Descent(70/999): loss=5.59464907325276, w0=-0.39826255330354116, w1=0.8416699914960846\n",
      "Gradient Descent(71/999): loss=5.590623262687323, w0=-0.40456716522113934, w1=0.8423033581481884\n",
      "Gradient Descent(72/999): loss=5.586611952859217, w0=-0.4108604124716399, w1=0.8429355830962117\n",
      "Gradient Descent(73/999): loss=5.5826150915376225, w0=-0.4171423155409443, w1=0.8435666683981857\n",
      "Gradient Descent(74/999): loss=5.578632626679854, w0=-0.4234128948780261, w1=0.8441966161084314\n",
      "Gradient Descent(75/999): loss=5.574664506430677, w0=-0.4296721708949977, w1=0.844825428277567\n",
      "Gradient Descent(76/999): loss=5.570710679121643, w0=-0.4359201639671767, w1=0.8454531069525142\n",
      "Gradient Descent(77/999): loss=5.566771093270403, w0=-0.44215689443315226, w1=0.8460796541765048\n",
      "Gradient Descent(78/999): loss=5.562845697580051, w0=-0.44838238259485125, w1=0.8467050719890875\n",
      "Gradient Descent(79/999): loss=5.558934440938442, w0=-0.45459664871760436, w1=0.8473293624261348\n",
      "Gradient Descent(80/999): loss=5.555037272417543, w0=-0.46079971303021217, w1=0.8479525275198488\n",
      "Gradient Descent(81/999): loss=5.551154141272755, w0=-0.46699159572501076, w1=0.8485745692987691\n",
      "Gradient Descent(82/999): loss=5.547284996942256, w0=-0.4731723169579377, w1=0.8491954897877779\n",
      "Gradient Descent(83/999): loss=5.5434297890463515, w0=-0.4793418968485975, w1=0.8498152910081078\n",
      "Gradient Descent(84/999): loss=5.539588467386808, w0=-0.4855003554803272, w1=0.8504339749773481\n",
      "Gradient Descent(85/999): loss=5.535760981946204, w0=-0.49164771290026166, w1=0.8510515437094506\n",
      "Gradient Descent(86/999): loss=5.531947282887275, w0=-0.49778398911939886, w1=0.8516679992147372\n",
      "Gradient Descent(87/999): loss=5.5281473205522715, w0=-0.503909204112665, w1=0.8522833434999061\n",
      "Gradient Descent(88/999): loss=5.524361045462305, w0=-0.5100233778189798, w1=0.8528975785680377\n",
      "Gradient Descent(89/999): loss=5.520588408316713, w0=-0.5161265301413208, w1=0.8535107064186024\n",
      "Gradient Descent(90/999): loss=5.5168293599924025, w0=-0.5222186809467888, w1=0.8541227290474656\n",
      "Gradient Descent(91/999): loss=5.513083851543224, w0=-0.5282998500666721, w1=0.8547336484468955\n",
      "Gradient Descent(92/999): loss=5.5093518341993315, w0=-0.5343700572965112, w1=0.8553434666055688\n",
      "Gradient Descent(93/999): loss=5.5056332593665385, w0=-0.5404293223961634, w1=0.8559521855085775\n",
      "Gradient Descent(94/999): loss=5.501928078625699, w0=-0.5464776650898667, w1=0.8565598071374354\n",
      "Gradient Descent(95/999): loss=5.498236243732065, w0=-0.5525151050663046, w1=0.8571663334700841\n",
      "Gradient Descent(96/999): loss=5.494557706614663, w0=-0.5585416619786696, w1=0.8577717664809001\n",
      "Gradient Descent(97/999): loss=5.490892419375678, w0=-0.5645573554447275, w1=0.8583761081407008\n",
      "Gradient Descent(98/999): loss=5.487240334289806, w0=-0.5705622050468813, w1=0.8589793604167508\n",
      "Gradient Descent(99/999): loss=5.483601403803652, w0=-0.5765562303322346, w1=0.8595815252727688\n",
      "Gradient Descent(100/999): loss=5.479975580535113, w0=-0.5825394508126557, w1=0.8601826046689336\n",
      "Gradient Descent(101/999): loss=5.476362817272741, w0=-0.5885118859648408, w1=0.8607826005618906\n",
      "Gradient Descent(102/999): loss=5.472763066975151, w0=-0.5944735552303777, w1=0.8613815149047582\n",
      "Gradient Descent(103/999): loss=5.469176282770398, w0=-0.6004244780158084, w1=0.861979349647134\n",
      "Gradient Descent(104/999): loss=5.465602417955358, w0=-0.6063646736926933, w1=0.8625761067351013\n",
      "Gradient Descent(105/999): loss=5.462041425995138, w0=-0.6122941615976732, w1=0.8631717881112356\n",
      "Gradient Descent(106/999): loss=5.4584932605224585, w0=-0.6182129610325332, w1=0.8637663957146104\n",
      "Gradient Descent(107/999): loss=5.454957875337047, w0=-0.6241210912642647, w1=0.864359931480804\n",
      "Gradient Descent(108/999): loss=5.451435224405051, w0=-0.6300185715251289, w1=0.8649523973419058\n",
      "Gradient Descent(109/999): loss=5.447925261858424, w0=-0.6359054210127185, w1=0.865543795226522\n",
      "Gradient Descent(110/999): loss=5.444427941994333, w0=-0.6417816588900211, w1=0.8661341270597827\n",
      "Gradient Descent(111/999): loss=5.440943219274565, w0=-0.6476473042854811, w1=0.8667233947633476\n",
      "Gradient Descent(112/999): loss=5.437471048324934, w0=-0.6535023762930621, w1=0.8673116002554123\n",
      "Gradient Descent(113/999): loss=5.434011383934687, w0=-0.6593468939723086, w1=0.8678987454507151\n",
      "Gradient Descent(114/999): loss=5.430564181055919, w0=-0.6651808763484091, w1=0.8684848322605423\n",
      "Gradient Descent(115/999): loss=5.427129394802985, w0=-0.6710043424122568, w1=0.8690698625927351\n",
      "Gradient Descent(116/999): loss=5.423706980451919, w0=-0.6768173111205124, w1=0.8696538383516959\n",
      "Gradient Descent(117/999): loss=5.420296893439838, w0=-0.682619801395665, w1=0.8702367614383937\n",
      "Gradient Descent(118/999): loss=5.416899089364382, w0=-0.6884118321260945, w1=0.8708186337503714\n",
      "Gradient Descent(119/999): loss=5.413513523983122, w0=-0.6941934221661324, w1=0.8713994571817509\n",
      "Gradient Descent(120/999): loss=5.4101401532129865, w0=-0.6999645903361237, w1=0.8719792336232401\n",
      "Gradient Descent(121/999): loss=5.406778933129694, w0=-0.7057253554224877, w1=0.8725579649621387\n",
      "Gradient Descent(122/999): loss=5.403429819967165, w0=-0.7114757361777795, w1=0.873135653082344\n",
      "Gradient Descent(123/999): loss=5.400092770116975, w0=-0.7172157513207509, w1=0.8737122998643578\n",
      "Gradient Descent(124/999): loss=5.396767740127768, w0=-0.7229454195364113, w1=0.874287907185292\n",
      "Gradient Descent(125/999): loss=5.393454686704697, w0=-0.7286647594760888, w1=0.8748624769188748\n",
      "Gradient Descent(126/999): loss=5.390153566708862, w0=-0.7343737897574903, w1=0.8754360109354568\n",
      "Gradient Descent(127/999): loss=5.386864337156746, w0=-0.7400725289647629, w1=0.876008511102017\n",
      "Gradient Descent(128/999): loss=5.383586955219661, w0=-0.7457609956485538, w1=0.8765799792821695\n",
      "Gradient Descent(129/999): loss=5.380321378223178, w0=-0.7514392083260708, w1=0.8771504173361683\n",
      "Gradient Descent(130/999): loss=5.377067563646582, w0=-0.7571071854811429, w1=0.8777198271209147\n",
      "Gradient Descent(131/999): loss=5.373825469122315, w0=-0.7627649455642799, w1=0.8782882104899624\n",
      "Gradient Descent(132/999): loss=5.37059505243543, w0=-0.7684125069927331, w1=0.8788555692935243\n",
      "Gradient Descent(133/999): loss=5.367376271523023, w0=-0.7740498881505549, w1=0.8794219053784776\n",
      "Gradient Descent(134/999): loss=5.364169084473713, w0=-0.7796771073886584, w1=0.8799872205883708\n",
      "Gradient Descent(135/999): loss=5.360973449527068, w0=-0.7852941830248777, w1=0.8805515167634288\n",
      "Gradient Descent(136/999): loss=5.357789325073084, w0=-0.7909011333440273, w1=0.8811147957405598\n",
      "Gradient Descent(137/999): loss=5.354616669651634, w0=-0.7964979765979614, w1=0.8816770593533604\n",
      "Gradient Descent(138/999): loss=5.3514554419519165, w0=-0.8020847310056334, w1=0.882238309432122\n",
      "Gradient Descent(139/999): loss=5.348305600811941, w0=-0.8076614147531554, w1=0.8827985478038369\n",
      "Gradient Descent(140/999): loss=5.34516710521798, w0=-0.8132280459938575, w1=0.8833577762922041\n",
      "Gradient Descent(141/999): loss=5.342039914304029, w0=-0.8187846428483462, w1=0.883915996717635\n",
      "Gradient Descent(142/999): loss=5.338923987351282, w0=-0.8243312234045643, w1=0.8844732108972596\n",
      "Gradient Descent(143/999): loss=5.335819283787601, w0=-0.8298678057178493, w1=0.8850294206449325\n",
      "Gradient Descent(144/999): loss=5.332725763186988, w0=-0.8353944078109922, w1=0.8855846277712385\n",
      "Gradient Descent(145/999): loss=5.329643385269053, w0=-0.8409110476742958, w1=0.8861388340834986\n",
      "Gradient Descent(146/999): loss=5.326572109898499, w0=-0.8464177432656342, w1=0.8866920413857761\n",
      "Gradient Descent(147/999): loss=5.323511897084589, w0=-0.8519145125105105, w1=0.8872442514788821\n",
      "Gradient Descent(148/999): loss=5.32046270698063, w0=-0.8574013733021153, w1=0.8877954661603819\n",
      "Gradient Descent(149/999): loss=5.317424499883461, w0=-0.862878343501385, w1=0.8883456872246002\n",
      "Gradient Descent(150/999): loss=5.314397236232924, w0=-0.8683454409370602, w1=0.8888949164626272\n",
      "Gradient Descent(151/999): loss=5.311380876611356, w0=-0.8738026834057432, w1=0.8894431556623249\n",
      "Gradient Descent(152/999): loss=5.3083753817430726, w0=-0.8792500886719562, w1=0.8899904066083322\n",
      "Gradient Descent(153/999): loss=5.305380712493861, w0=-0.8846876744681994, w1=0.8905366710820709\n",
      "Gradient Descent(154/999): loss=5.302396829870465, w0=-0.8901154584950083, w1=0.891081950861752\n",
      "Gradient Descent(155/999): loss=5.2994236950200815, w0=-0.8955334584210115, w1=0.8916262477223807\n",
      "Gradient Descent(156/999): loss=5.296461269229852, w0=-0.9009416918829883, w1=0.892169563435763\n",
      "Gradient Descent(157/999): loss=5.293509513926361, w0=-0.9063401764859259, w1=0.8927118997705107\n",
      "Gradient Descent(158/999): loss=5.290568390675129, w0=-0.9117289298030768, w1=0.8932532584920476\n",
      "Gradient Descent(159/999): loss=5.287637861180119, w0=-0.9171079693760162, w1=0.8937936413626153\n",
      "Gradient Descent(160/999): loss=5.284717887283231, w0=-0.9224773127146988, w1=0.8943330501412785\n",
      "Gradient Descent(161/999): loss=5.281808430963812, w0=-0.9278369772975159, w1=0.8948714865839313\n",
      "Gradient Descent(162/999): loss=5.278909454338152, w0=-0.9331869805713524, w1=0.8954089524433027\n",
      "Gradient Descent(163/999): loss=5.276020919659001, w0=-0.9385273399516436, w1=0.8959454494689617\n",
      "Gradient Descent(164/999): loss=5.27314278931507, w0=-0.9438580728224316, w1=0.8964809794073243\n",
      "Gradient Descent(165/999): loss=5.270275025830545, w0=-0.9491791965364222, w1=0.8970155440016577\n",
      "Gradient Descent(166/999): loss=5.267417591864592, w0=-0.9544907284150413, w1=0.8975491449920872\n",
      "Gradient Descent(167/999): loss=5.264570450210886, w0=-0.9597926857484913, w1=0.898081784115601\n",
      "Gradient Descent(168/999): loss=5.261733563797109, w0=-0.9650850857958073, w1=0.8986134631060566\n",
      "Gradient Descent(169/999): loss=5.2589068956844836, w0=-0.9703679457849134, w1=0.8991441836941857\n",
      "Gradient Descent(170/999): loss=5.256090409067274, w0=-0.9756412829126785, w1=0.8996739476076003\n",
      "Gradient Descent(171/999): loss=5.253284067272322, w0=-0.9809051143449727, w1=0.9002027565707984\n",
      "Gradient Descent(172/999): loss=5.250487833758565, w0=-0.9861594572167232, w1=0.900730612305169\n",
      "Gradient Descent(173/999): loss=5.2477016721165555, w0=-0.9914043286319691, w1=0.9012575165289988\n",
      "Gradient Descent(174/999): loss=5.244925546067995, w0=-0.9966397456639188, w1=0.9017834709574764\n",
      "Gradient Descent(175/999): loss=5.242159419465253, w0=-1.0018657253550038, w1=0.902308477302699\n",
      "Gradient Descent(176/999): loss=5.2394032562909025, w0=-1.0070822847169356, w1=0.9028325372736775\n",
      "Gradient Descent(177/999): loss=5.236657020657252, w0=-1.0122894407307599, w1=0.9033556525763423\n",
      "Gradient Descent(178/999): loss=5.2339206768058695, w0=-1.0174872103469128, w1=0.9038778249135484\n",
      "Gradient Descent(179/999): loss=5.2311941891071285, w0=-1.0226756104852754, w1=0.9043990559850815\n",
      "Gradient Descent(180/999): loss=5.228477522059737, w0=-1.0278546580352295, w1=0.9049193474876631\n",
      "Gradient Descent(181/999): loss=5.225770640290271, w0=-1.0330243698557116, w1=0.9054387011149563\n",
      "Gradient Descent(182/999): loss=5.223073508552729, w0=-1.0381847627752687, w1=0.9059571185575711\n",
      "Gradient Descent(183/999): loss=5.220386091728055, w0=-1.043335853592113, w1=0.9064746015030702\n",
      "Gradient Descent(184/999): loss=5.217708354823697, w0=-1.0484776590741753, w1=0.9069911516359741\n",
      "Gradient Descent(185/999): loss=5.215040262973136, w0=-1.053610195959162, w1=0.907506770637767\n",
      "Gradient Descent(186/999): loss=5.212381781435449, w0=-1.058733480954607, w1=0.9080214601869018\n",
      "Gradient Descent(187/999): loss=5.2097328755948435, w0=-1.0638475307379278, w1=0.9085352219588061\n",
      "Gradient Descent(188/999): loss=5.207093510960209, w0=-1.0689523619564794, w1=0.9090480576258873\n",
      "Gradient Descent(189/999): loss=5.204463653164672, w0=-1.0740479912276077, w1=0.9095599688575379\n",
      "Gradient Descent(190/999): loss=5.20184326796515, w0=-1.079134435138705, w1=0.9100709573201414\n",
      "Gradient Descent(191/999): loss=5.199232321241896, w0=-1.084211710247263, w1=0.9105810246770776\n",
      "Gradient Descent(192/999): loss=5.196630778998063, w0=-1.0892798330809266, w1=0.9110901725887276\n",
      "Gradient Descent(193/999): loss=5.19403860735926, w0=-1.0943388201375484, w1=0.9115984027124796\n",
      "Gradient Descent(194/999): loss=5.191455772573109, w0=-1.099388687885242, w1=0.9121057167027342\n",
      "Gradient Descent(195/999): loss=5.188882241008802, w0=-1.1044294527624352, w1=0.91261211621091\n",
      "Gradient Descent(196/999): loss=5.1863179791566765, w0=-1.1094611311779248, w1=0.9131176028854483\n",
      "Gradient Descent(197/999): loss=5.183762953627759, w0=-1.1144837395109284, w1=0.9136221783718195\n",
      "Gradient Descent(198/999): loss=5.181217131153349, w0=-1.119497294111139, w1=0.9141258443125273\n",
      "Gradient Descent(199/999): loss=5.1786804785845755, w0=-1.1245018112987772, w1=0.9146286023471152\n",
      "Gradient Descent(200/999): loss=5.176152962891967, w0=-1.1294973073646455, w1=0.9151304541121706\n",
      "Gradient Descent(201/999): loss=5.173634551165022, w0=-1.1344837985701801, w1=0.9156314012413316\n",
      "Gradient Descent(202/999): loss=5.171125210611782, w0=-1.1394613011475045, w1=0.9161314453652908\n",
      "Gradient Descent(203/999): loss=5.168624908558403, w0=-1.1444298312994825, w1=0.9166305881118018\n",
      "Gradient Descent(204/999): loss=5.166133612448731, w0=-1.1493894051997706, w1=0.9171288311056836\n",
      "Gradient Descent(205/999): loss=5.163651289843876, w0=-1.1543400389928706, w1=0.9176261759688266\n",
      "Gradient Descent(206/999): loss=5.161177908421789, w0=-1.1592817487941822, w1=0.9181226243201975\n",
      "Gradient Descent(207/999): loss=5.15871343597685, w0=-1.1642145506900559, w1=0.9186181777758449\n",
      "Gradient Descent(208/999): loss=5.156257840419434, w0=-1.1691384607378448, w1=0.9191128379489037\n",
      "Gradient Descent(209/999): loss=5.153811089775505, w0=-1.1740534949659571, w1=0.9196066064496018\n",
      "Gradient Descent(210/999): loss=5.151373152186197, w0=-1.1789596693739082, w1=0.9200994848852637\n",
      "Gradient Descent(211/999): loss=5.148943995907395, w0=-1.183856999932373, w1=0.9205914748603171\n",
      "Gradient Descent(212/999): loss=5.146523589309322, w0=-1.1887455025832374, w1=0.9210825779762974\n",
      "Gradient Descent(213/999): loss=5.144111900876139, w0=-1.193625193239651, w1=0.921572795831853\n",
      "Gradient Descent(214/999): loss=5.141708899205516, w0=-1.1984960877860782, w1=0.9220621300227506\n",
      "Gradient Descent(215/999): loss=5.139314553008234, w0=-1.20335820207835, w1=0.9225505821418805\n",
      "Gradient Descent(216/999): loss=5.136928831107778, w0=-1.2082115519437158, w1=0.9230381537792615\n",
      "Gradient Descent(217/999): loss=5.134551702439932, w0=-1.2130561531808948, w1=0.9235248465220464\n",
      "Gradient Descent(218/999): loss=5.132183136052365, w0=-1.2178920215601279, w1=0.9240106619545269\n",
      "Gradient Descent(219/999): loss=5.129823101104237, w0=-1.222719172823228, w1=0.9244956016581389\n",
      "Gradient Descent(220/999): loss=5.127471566865799, w0=-1.2275376226836328, w1=0.9249796672114675\n",
      "Gradient Descent(221/999): loss=5.12512850271798, w0=-1.2323473868264538, w1=0.9254628601902526\n",
      "Gradient Descent(222/999): loss=5.122793878152007, w0=-1.2371484809085296, w1=0.9259451821673931\n",
      "Gradient Descent(223/999): loss=5.120467662768992, w0=-1.2419409205584753, w1=0.9264266347129532\n",
      "Gradient Descent(224/999): loss=5.118149826279542, w0=-1.246724721376734, w1=0.9269072193941664\n",
      "Gradient Descent(225/999): loss=5.115840338503368, w0=-1.251499898935628, w1=0.9273869377754415\n",
      "Gradient Descent(226/999): loss=5.113539169368884, w0=-1.2562664687794083, w1=0.9278657914183671\n",
      "Gradient Descent(227/999): loss=5.111246288912825, w0=-1.2610244464243061, w1=0.928343781881717\n",
      "Gradient Descent(228/999): loss=5.108961667279847, w0=-1.2657738473585836, w1=0.928820910721455\n",
      "Gradient Descent(229/999): loss=5.10668527472215, w0=-1.2705146870425832, w1=0.9292971794907403\n",
      "Gradient Descent(230/999): loss=5.104417081599077, w0=-1.275246980908779, w1=0.9297725897399323\n",
      "Gradient Descent(231/999): loss=5.102157058376736, w0=-1.2799707443618262, w1=0.9302471430165956\n",
      "Gradient Descent(232/999): loss=5.099905175627619, w0=-1.2846859927786123, w1=0.9307208408655054\n",
      "Gradient Descent(233/999): loss=5.097661404030211, w0=-1.2893927415083057, w1=0.9311936848286523\n",
      "Gradient Descent(234/999): loss=5.095425714368609, w0=-1.294091005872407, w1=0.931665676445247\n",
      "Gradient Descent(235/999): loss=5.093198077532151, w0=-1.2987808011647983, w1=0.932136817251726\n",
      "Gradient Descent(236/999): loss=5.090978464515021, w0=-1.3034621426517927, w1=0.932607108781756\n",
      "Gradient Descent(237/999): loss=5.088766846415888, w0=-1.3081350455721845, w1=0.9330765525662391\n",
      "Gradient Descent(238/999): loss=5.086563194437517, w0=-1.3127995251372988, w1=0.9335451501333181\n",
      "Gradient Descent(239/999): loss=5.0843674798864, w0=-1.3174555965310404, w1=0.9340129030083807\n",
      "Gradient Descent(240/999): loss=5.082179674172386, w0=-1.322103274909944, w1=0.9344798127140652\n",
      "Gradient Descent(241/999): loss=5.079999748808296, w0=-1.326742575403223, w1=0.9349458807702654\n",
      "Gradient Descent(242/999): loss=5.0778276754095675, w0=-1.3313735131128188, w1=0.9354111086941351\n",
      "Gradient Descent(243/999): loss=5.075663425693872, w0=-1.3359961031134509, w1=0.9358754980000933\n",
      "Gradient Descent(244/999): loss=5.0735069714807555, w0=-1.340610360452664, w1=0.9363390501998291\n",
      "Gradient Descent(245/999): loss=5.071358284691268, w0=-1.345216300150879, w1=0.9368017668023066\n",
      "Gradient Descent(246/999): loss=5.069217337347596, w0=-1.3498139372014408, w1=0.9372636493137702\n",
      "Gradient Descent(247/999): loss=5.067084101572705, w0=-1.3544032865706674, w1=0.9377246992377486\n",
      "Gradient Descent(248/999): loss=5.064958549589969, w0=-1.3589843631978986, w1=0.9381849180750605\n",
      "Gradient Descent(249/999): loss=5.06284065372281, w0=-1.3635571819955445, w1=0.9386443073238195\n",
      "Gradient Descent(250/999): loss=5.060730386394342, w0=-1.3681217578491343, w1=0.9391028684794384\n",
      "Gradient Descent(251/999): loss=5.05862772012701, w0=-1.3726781056173643, w1=0.9395606030346343\n",
      "Gradient Descent(252/999): loss=5.056532627542231, w0=-1.3772262401321467, w1=0.9400175124794337\n",
      "Gradient Descent(253/999): loss=5.054445081360037, w0=-1.3817661761986582, w1=0.9404735983011773\n",
      "Gradient Descent(254/999): loss=5.052365054398719, w0=-1.3862979285953871, w1=0.9409288619845243\n",
      "Gradient Descent(255/999): loss=5.050292519574479, w0=-1.3908215120741825, w1=0.9413833050114583\n",
      "Gradient Descent(256/999): loss=5.0482274499010735, w0=-1.3953369413603016, w1=0.9418369288612908\n",
      "Gradient Descent(257/999): loss=5.046169818489457, w0=-1.3998442311524584, w1=0.9422897350106672\n",
      "Gradient Descent(258/999): loss=5.044119598547442, w0=-1.4043433961228702, w1=0.9427417249335708\n",
      "Gradient Descent(259/999): loss=5.042076763379342, w0=-1.408834450917307, w1=0.9431929001013282\n",
      "Gradient Descent(260/999): loss=5.040041286385627, w0=-1.4133174101551382, w1=0.9436432619826134\n",
      "Gradient Descent(261/999): loss=5.038013141062576, w0=-1.4177922884293803, w1=0.9440928120434534\n",
      "Gradient Descent(262/999): loss=5.035992301001938, w0=-1.4222591003067444, w1=0.9445415517472323\n",
      "Gradient Descent(263/999): loss=5.033978739890578, w0=-1.4267178603276838, w1=0.9449894825546963\n",
      "Gradient Descent(264/999): loss=5.03197243151014, w0=-1.4311685830064411, w1=0.9454366059239585\n",
      "Gradient Descent(265/999): loss=5.029973349736707, w0=-1.4356112828310958, w1=0.9458829233105036\n",
      "Gradient Descent(266/999): loss=5.027981468540455, w0=-1.4400459742636114, w1=0.9463284361671929\n",
      "Gradient Descent(267/999): loss=5.025996761985324, w0=-1.444472671739882, w1=0.9467731459442682\n",
      "Gradient Descent(268/999): loss=5.024019204228667, w0=-1.4488913896697795, w1=0.9472170540893577\n",
      "Gradient Descent(269/999): loss=5.022048769520927, w0=-1.4533021424372012, w1=0.9476601620474799\n",
      "Gradient Descent(270/999): loss=5.020085432205294, w0=-1.4577049444001156, w1=0.9481024712610484\n",
      "Gradient Descent(271/999): loss=5.018129166717367, w0=-1.4620998098906095, w1=0.9485439831698768\n",
      "Gradient Descent(272/999): loss=5.016179947584834, w0=-1.466486753214935, w1=0.9489846992111832\n",
      "Gradient Descent(273/999): loss=5.014237749427129, w0=-1.4708657886535559, w1=0.9494246208195952\n",
      "Gradient Descent(274/999): loss=5.012302546955106, w0=-1.4752369304611936, w1=0.9498637494271542\n",
      "Gradient Descent(275/999): loss=5.010374314970709, w0=-1.4796001928668747, w1=0.9503020864633202\n",
      "Gradient Descent(276/999): loss=5.008453028366642, w0=-1.483955590073976, w1=0.9507396333549765\n",
      "Gradient Descent(277/999): loss=5.006538662126046, w0=-1.4883031362602717, w1=0.9511763915264342\n",
      "Gradient Descent(278/999): loss=5.004631191322176, w0=-1.492642845577979, w1=0.9516123623994371\n",
      "Gradient Descent(279/999): loss=5.002730591118061, w0=-1.4969747321538045, w1=0.9520475473931662\n",
      "Gradient Descent(280/999): loss=5.0008368367662, w0=-1.5012988100889901, w1=0.9524819479242441\n",
      "Gradient Descent(281/999): loss=4.998949903608226, w0=-1.5056150934593588, w1=0.9529155654067399\n",
      "Gradient Descent(282/999): loss=4.997069767074596, w0=-1.5099235963153605, w1=0.9533484012521737\n",
      "Gradient Descent(283/999): loss=4.995196402684257, w0=-1.5142243326821179, w1=0.9537804568695212\n",
      "Gradient Descent(284/999): loss=4.993329786044343, w0=-1.5185173165594719, w1=0.9542117336652183\n",
      "Gradient Descent(285/999): loss=4.991469892849845, w0=-1.5228025619220278, w1=0.9546422330431656\n",
      "Gradient Descent(286/999): loss=4.989616698883301, w0=-1.5270800827192, w1=0.9550719564047332\n",
      "Gradient Descent(287/999): loss=4.987770180014479, w0=-1.5313498928752574, w1=0.955500905148765\n",
      "Gradient Descent(288/999): loss=4.985930312200061, w0=-1.53561200628937, w1=0.9559290806715831\n",
      "Gradient Descent(289/999): loss=4.984097071483332, w0=-1.539866436835652, w1=0.956356484366993\n",
      "Gradient Descent(290/999): loss=4.982270433993872, w0=-1.5441131983632095, w1=0.9567831176262876\n",
      "Gradient Descent(291/999): loss=4.9804503759472345, w0=-1.5483523046961833, w1=0.9572089818382518\n",
      "Gradient Descent(292/999): loss=4.978636873644648, w0=-1.5525837696337952, w1=0.957634078389167\n",
      "Gradient Descent(293/999): loss=4.976829903472698, w0=-1.5568076069503927, w1=0.9580584086628159\n",
      "Gradient Descent(294/999): loss=4.975029441903031, w0=-1.5610238303954933, w1=0.9584819740404867\n",
      "Gradient Descent(295/999): loss=4.973235465492034, w0=-1.56523245369383, w1=0.9589047759009777\n",
      "Gradient Descent(296/999): loss=4.971447950880541, w0=-1.5694334905453957, w1=0.9593268156206016\n",
      "Gradient Descent(297/999): loss=4.969666874793522, w0=-1.5736269546254877, w1=0.9597480945731905\n",
      "Gradient Descent(298/999): loss=4.967892214039784, w0=-1.577812859584752, w1=0.9601686141300999\n",
      "Gradient Descent(299/999): loss=4.966123945511668, w0=-1.5819912190492285, w1=0.9605883756602133\n",
      "Gradient Descent(300/999): loss=4.964362046184745, w0=-1.5861620466203943, w1=0.9610073805299465\n",
      "Gradient Descent(301/999): loss=4.962606493117519, w0=-1.590325355875209, w1=0.9614256301032524\n",
      "Gradient Descent(302/999): loss=4.96085726345113, w0=-1.5944811603661582, w1=0.9618431257416252\n",
      "Gradient Descent(303/999): loss=4.959114334409055, w0=-1.5986294736212978, w1=0.9622598688041049\n",
      "Gradient Descent(304/999): loss=4.957377683296804, w0=-1.6027703091442982, w1=0.9626758606472816\n",
      "Gradient Descent(305/999): loss=4.955647287501639, w0=-1.6069036804144883, w1=0.9630911026253001\n",
      "Gradient Descent(306/999): loss=4.95392312449227, w0=-1.6110296008868987, w1=0.9635055960898642\n",
      "Gradient Descent(307/999): loss=4.9522051718185605, w0=-1.6151480839923065, w1=0.963919342390241\n",
      "Gradient Descent(308/999): loss=4.95049340711124, w0=-1.6192591431372785, w1=0.9643323428732659\n",
      "Gradient Descent(309/999): loss=4.948787808081611, w0=-1.6233627917042146, w1=0.9647445988833457\n",
      "Gradient Descent(310/999): loss=4.9470883525212574, w0=-1.6274590430513918, w1=0.9651561117624645\n",
      "Gradient Descent(311/999): loss=4.94539501830176, w0=-1.6315479105130075, w1=0.965566882850187\n",
      "Gradient Descent(312/999): loss=4.943707783374399, w0=-1.635629407399223, w1=0.9659769134836632\n",
      "Gradient Descent(313/999): loss=4.942026625769878, w0=-1.639703546996207, w1=0.966386204997633\n",
      "Gradient Descent(314/999): loss=4.9403515235980295, w0=-1.6437703425661778, w1=0.9667947587244299\n",
      "Gradient Descent(315/999): loss=4.938682455047537, w0=-1.6478298073474482, w1=0.9672025759939861\n",
      "Gradient Descent(316/999): loss=4.937019398385642, w0=-1.6518819545544672, w1=0.9676096581338364\n",
      "Gradient Descent(317/999): loss=4.93536233195787, w0=-1.6559267973778633, w1=0.9680160064691224\n",
      "Gradient Descent(318/999): loss=4.933711234187743, w0=-1.6599643489844882, w1=0.9684216223225973\n",
      "Gradient Descent(319/999): loss=4.932066083576499, w0=-1.6639946225174587, w1=0.9688265070146297\n",
      "Gradient Descent(320/999): loss=4.930426858702819, w0=-1.6680176310961998, w1=0.969230661863208\n",
      "Gradient Descent(321/999): loss=4.928793538222535, w0=-1.672033387816488, w1=0.9696340881839453\n",
      "Gradient Descent(322/999): loss=4.927166100868363, w0=-1.6760419057504927, w1=0.9700367872900825\n",
      "Gradient Descent(323/999): loss=4.925544525449624, w0=-1.68004319794682, w1=0.9704387604924937\n",
      "Gradient Descent(324/999): loss=4.923928790851962, w0=-1.6840372774305543, w1=0.9708400090996899\n",
      "Gradient Descent(325/999): loss=4.922318876037079, w0=-1.6880241572033015, w1=0.9712405344178235\n",
      "Gradient Descent(326/999): loss=4.920714760042453, w0=-1.6920038502432302, w1=0.9716403377506925\n",
      "Gradient Descent(327/999): loss=4.919116421981067, w0=-1.6959763695051149, w1=0.9720394203997442\n",
      "Gradient Descent(328/999): loss=4.91752384104114, w0=-1.6999417279203781, w1=0.9724377836640803\n",
      "Gradient Descent(329/999): loss=4.9159369964858515, w0=-1.703899938397132, w1=0.9728354288404608\n",
      "Gradient Descent(330/999): loss=4.914355867653076, w0=-1.7078510138202208, w1=0.973232357223308\n",
      "Gradient Descent(331/999): loss=4.9127804339551115, w0=-1.7117949670512622, w1=0.9736285701047109\n",
      "Gradient Descent(332/999): loss=4.911210674878411, w0=-1.7157318109286899, w1=0.9740240687744295\n",
      "Gradient Descent(333/999): loss=4.9096465699833125, w0=-1.719661558267795, w1=0.9744188545198987\n",
      "Gradient Descent(334/999): loss=4.908088098903785, w0=-1.7235842218607678, w1=0.9748129286262328\n",
      "Gradient Descent(335/999): loss=4.9065352413471475, w0=-1.7274998144767395, w1=0.9752062923762296\n",
      "Gradient Descent(336/999): loss=4.904987977093815, w0=-1.7314083488618237, w1=0.9755989470503743\n",
      "Gradient Descent(337/999): loss=4.903446285997032, w0=-1.735309837739158, w1=0.9759908939268442\n",
      "Gradient Descent(338/999): loss=4.901910147982609, w0=-1.7392042938089451, w1=0.9763821342815124\n",
      "Gradient Descent(339/999): loss=4.9003795430486665, w0=-1.7430917297484947, w1=0.9767726693879522\n",
      "Gradient Descent(340/999): loss=4.898854451265366, w0=-1.746972158212264, w1=0.9771625005174409\n",
      "Gradient Descent(341/999): loss=4.897334852774656, w0=-1.7508455918318995, w1=0.9775516289389647\n",
      "Gradient Descent(342/999): loss=4.895820727790017, w0=-1.7547120432162782, w1=0.977940055919222\n",
      "Gradient Descent(343/999): loss=4.894312056596192, w0=-1.7585715249515481, w1=0.9783277827226277\n",
      "Gradient Descent(344/999): loss=4.892808819548943, w0=-1.7624240496011696, w1=0.9787148106113177\n",
      "Gradient Descent(345/999): loss=4.89131099707479, w0=-1.7662696297059564, w1=0.9791011408451529\n",
      "Gradient Descent(346/999): loss=4.889818569670749, w0=-1.7701082777841157, w1=0.979486774681723\n",
      "Gradient Descent(347/999): loss=4.888331517904091, w0=-1.7739400063312898, w1=0.9798717133763506\n",
      "Gradient Descent(348/999): loss=4.88684982241208, w0=-1.7777648278205964, w1=0.9802559581820959\n",
      "Gradient Descent(349/999): loss=4.885373463901725, w0=-1.7815827547026692, w1=0.98063951034976\n",
      "Gradient Descent(350/999): loss=4.883902423149523, w0=-1.7853937994056983, w1=0.9810223711278895\n",
      "Gradient Descent(351/999): loss=4.88243668100122, w0=-1.789197974335471, w1=0.98140454176278\n",
      "Gradient Descent(352/999): loss=4.880976218371547, w0=-1.7929952918754115, w1=0.9817860234984812\n",
      "Gradient Descent(353/999): loss=4.879521016243986, w0=-1.7967857643866225, w1=0.9821668175767997\n",
      "Gradient Descent(354/999): loss=4.878071055670509, w0=-1.800569404207924, w1=0.9825469252373037\n",
      "Gradient Descent(355/999): loss=4.876626317771342, w0=-1.8043462236558945, w1=0.9829263477173273\n",
      "Gradient Descent(356/999): loss=4.875186783734713, w0=-1.80811623502491, w1=0.983305086251974\n",
      "Gradient Descent(357/999): loss=4.87375243481661, w0=-1.8118794505871856, w1=0.9836831420741207\n",
      "Gradient Descent(358/999): loss=4.872323252340535, w0=-1.8156358825928138, w1=0.9840605164144222\n",
      "Gradient Descent(359/999): loss=4.870899217697262, w0=-1.8193855432698058, w1=0.9844372105013148\n",
      "Gradient Descent(360/999): loss=4.869480312344593, w0=-1.82312844482413, w1=0.9848132255610205\n",
      "Gradient Descent(361/999): loss=4.868066517807122, w0=-1.826864599439753, w1=0.9851885628175506\n",
      "Gradient Descent(362/999): loss=4.866657815675988, w0=-1.830594019278678, w1=0.9855632234927107\n",
      "Gradient Descent(363/999): loss=4.865254187608632, w0=-1.8343167164809855, w1=0.9859372088061034\n",
      "Gradient Descent(364/999): loss=4.863855615328574, w0=-1.8380327031648722, w1=0.9863105199751329\n",
      "Gradient Descent(365/999): loss=4.862462080625159, w0=-1.8417419914266904, w1=0.986683158215009\n",
      "Gradient Descent(366/999): loss=4.861073565353324, w0=-1.8454445933409878, w1=0.987055124738751\n",
      "Gradient Descent(367/999): loss=4.859690051433372, w0=-1.8491405209605465, w1=0.9874264207571918\n",
      "Gradient Descent(368/999): loss=4.858311520850715, w0=-1.8528297863164225, w1=0.9877970474789811\n",
      "Gradient Descent(369/999): loss=4.856937955655664, w0=-1.8565124014179843, w1=0.9881670061105906\n",
      "Gradient Descent(370/999): loss=4.855569337963178, w0=-1.8601883782529525, w1=0.9885362978563167\n",
      "Gradient Descent(371/999): loss=4.85420564995264, w0=-1.8638577287874387, w1=0.988904923918285\n",
      "Gradient Descent(372/999): loss=4.852846873867617, w0=-1.8675204649659847, w1=0.9892728854964544\n",
      "Gradient Descent(373/999): loss=4.85149299201564, w0=-1.8711765987116007, w1=0.9896401837886206\n",
      "Gradient Descent(374/999): loss=4.850143986767962, w0=-1.8748261419258045, w1=0.9900068199904203\n",
      "Gradient Descent(375/999): loss=4.8487998405593355, w0=-1.878469106488661, w1=0.9903727952953345\n",
      "Gradient Descent(376/999): loss=4.847460535887785, w0=-1.882105504258819, w1=0.9907381108946934\n",
      "Gradient Descent(377/999): loss=4.84612605531437, w0=-1.885735347073552, w1=0.9911027679776796\n",
      "Gradient Descent(378/999): loss=4.844796381462968, w0=-1.8893586467487953, w1=0.9914667677313317\n",
      "Gradient Descent(379/999): loss=4.843471497020046, w0=-1.8929754150791847, w1=0.9918301113405489\n",
      "Gradient Descent(380/999): loss=4.842151384734429, w0=-1.896585663838095, w1=0.9921927999880944\n",
      "Gradient Descent(381/999): loss=4.840836027417081, w0=-1.9001894047776786, w1=0.9925548348545994\n",
      "Gradient Descent(382/999): loss=4.839525407940884, w0=-1.9037866496289035, w1=0.9929162171185668\n",
      "Gradient Descent(383/999): loss=4.838219509240404, w0=-1.9073774101015915, w1=0.9932769479563754\n",
      "Gradient Descent(384/999): loss=4.836918314311682, w0=-1.910961697884456, w1=0.9936370285422832\n",
      "Gradient Descent(385/999): loss=4.8356218062120035, w0=-1.9145395246451407, w1=0.9939964600484315\n",
      "Gradient Descent(386/999): loss=4.834329968059677, w0=-1.9181109020302571, w1=0.9943552436448486\n",
      "Gradient Descent(387/999): loss=4.833042783033826, w0=-1.921675841665423, w1=0.9947133804994541\n",
      "Gradient Descent(388/999): loss=4.831760234374157, w0=-1.9252343551552993, w1=0.9950708717780621\n",
      "Gradient Descent(389/999): loss=4.830482305380745, w0=-1.9287864540836286, w1=0.9954277186443851\n",
      "Gradient Descent(390/999): loss=4.829208979413817, w0=-1.9323321500132729, w1=0.995783922260038\n",
      "Gradient Descent(391/999): loss=4.8279402398935405, w0=-1.9358714544862508, w1=0.9961394837845419\n",
      "Gradient Descent(392/999): loss=4.826676070299798, w0=-1.9394043790237754, w1=0.9964944043753273\n",
      "Gradient Descent(393/999): loss=4.825416454171979, w0=-1.9429309351262916, w1=0.9968486851877391\n",
      "Gradient Descent(394/999): loss=4.824161375108761, w0=-1.9464511342735138, w1=0.9972023273750387\n",
      "Gradient Descent(395/999): loss=4.822910816767899, w0=-1.9499649879244632, w1=0.9975553320884093\n",
      "Gradient Descent(396/999): loss=4.821664762866011, w0=-1.9534725075175046, w1=0.9979077004769586\n",
      "Gradient Descent(397/999): loss=4.8204231971783695, w0=-1.9569737044703845, w1=0.9982594336877233\n",
      "Gradient Descent(398/999): loss=4.819186103538687, w0=-1.9604685901802676, w1=0.998610532865672\n",
      "Gradient Descent(399/999): loss=4.817953465838902, w0=-1.9639571760237742, w1=0.9989609991537096\n",
      "Gradient Descent(400/999): loss=4.816725268028978, w0=-1.9674394733570169, w1=0.999310833692681\n",
      "Gradient Descent(401/999): loss=4.815501494116686, w0=-1.970915493515638, w1=0.9996600376213742\n",
      "Gradient Descent(402/999): loss=4.814282128167403, w0=-1.9743852478148467, w1=1.0000086120765248\n",
      "Gradient Descent(403/999): loss=4.8130671543039005, w0=-1.9778487475494546, w1=1.000356558192819\n",
      "Gradient Descent(404/999): loss=4.811856556706141, w0=-1.9813060039939139, w1=1.0007038771028982\n",
      "Gradient Descent(405/999): loss=4.810650319611066, w0=-1.984757028402353, w1=1.0010505699373613\n",
      "Gradient Descent(406/999): loss=4.809448427312396, w0=-1.9882018320086143, w1=1.0013966378247696\n",
      "Gradient Descent(407/999): loss=4.808250864160425, w0=-1.9916404260262899, w1=1.0017420818916503\n",
      "Gradient Descent(408/999): loss=4.807057614561818, w0=-1.9950728216487579, w1=1.0020869032624995\n",
      "Gradient Descent(409/999): loss=4.805868662979403, w0=-1.9984990300492198, w1=1.0024311030597866\n",
      "Gradient Descent(410/999): loss=4.804683993931975, w0=-2.001919062380736, w1=1.0027746824039572\n",
      "Gradient Descent(411/999): loss=4.80350359199409, w0=-2.0053329297762628, w1=1.0031176424134378\n",
      "Gradient Descent(412/999): loss=4.802327441795866, w0=-2.008740643348688, w1=1.0034599842046386\n",
      "Gradient Descent(413/999): loss=4.80115552802278, w0=-2.0121422141908676, w1=1.0038017088919569\n",
      "Gradient Descent(414/999): loss=4.799987835415477, w0=-2.015537653375661, w1=1.004142817587782\n",
      "Gradient Descent(415/999): loss=4.798824348769556, w0=-2.018926971955968, w1=1.0044833114024974\n",
      "Gradient Descent(416/999): loss=4.7976650529353915, w0=-2.0223101809647654, w1=1.004823191444485\n",
      "Gradient Descent(417/999): loss=4.796509932817918, w0=-2.025687291415141, w1=1.0051624588201293\n",
      "Gradient Descent(418/999): loss=4.795358973376449, w0=-2.0290583143003307, w1=1.00550111463382\n",
      "Gradient Descent(419/999): loss=4.794212159624466, w0=-2.0324232605937538, w1=1.005839159987956\n",
      "Gradient Descent(420/999): loss=4.793069476629436, w0=-2.0357821412490495, w1=1.0061765959829492\n",
      "Gradient Descent(421/999): loss=4.791930909512613, w0=-2.039134967200112, w1=1.006513423717228\n",
      "Gradient Descent(422/999): loss=4.790796443448837, w0=-2.042481749361125, w1=1.0068496442872403\n",
      "Gradient Descent(423/999): loss=4.7896660636663535, w0=-2.0458224986266003, w1=1.0071852587874581\n",
      "Gradient Descent(424/999): loss=4.788539755446615, w0=-2.0491572258714092, w1=1.0075202683103803\n",
      "Gradient Descent(425/999): loss=4.787417504124085, w0=-2.0524859419508217, w1=1.0078546739465364\n",
      "Gradient Descent(426/999): loss=4.786299295086054, w0=-2.055808657700539, w1=1.0081884767844902\n",
      "Gradient Descent(427/999): loss=4.785185113772447, w0=-2.0591253839367307, w1=1.008521677910843\n",
      "Gradient Descent(428/999): loss=4.784074945675636, w0=-2.0624361314560686, w1=1.008854278410238\n",
      "Gradient Descent(429/999): loss=4.7829687763402395, w0=-2.0657409110357627, w1=1.0091862793653623\n",
      "Gradient Descent(430/999): loss=4.781866591362954, w0=-2.069039733433596, w1=1.0095176818569525\n",
      "Gradient Descent(431/999): loss=4.780768376392351, w0=-2.07233260938796, w1=1.0098484869637963\n",
      "Gradient Descent(432/999): loss=4.779674117128695, w0=-2.075619549617888, w1=1.0101786957627366\n",
      "Gradient Descent(433/999): loss=4.7785837993237585, w0=-2.078900564823093, w1=1.0105083093286757\n",
      "Gradient Descent(434/999): loss=4.777497408780635, w0=-2.082175665684, w1=1.0108373287345784\n",
      "Gradient Descent(435/999): loss=4.7764149313535516, w0=-2.08544486286178, w1=1.0111657550514748\n",
      "Gradient Descent(436/999): loss=4.775336352947692, w0=-2.0887081669983885, w1=1.0114935893484647\n",
      "Gradient Descent(437/999): loss=4.774261659519007, w0=-2.091965588716597, w1=1.011820832692721\n",
      "Gradient Descent(438/999): loss=4.773190837074031, w0=-2.095217138620028, w1=1.0121474861494921\n",
      "Gradient Descent(439/999): loss=4.772123871669708, w0=-2.09846282729319, w1=1.0124735507821072\n",
      "Gradient Descent(440/999): loss=4.771060749413196, w0=-2.1017026653015125, w1=1.012799027651978\n",
      "Gradient Descent(441/999): loss=4.770001456461701, w0=-2.1049366631913795, w1=1.0131239178186033\n",
      "Gradient Descent(442/999): loss=4.768945979022286, w0=-2.108164831490164, w1=1.0134482223395718\n",
      "Gradient Descent(443/999): loss=4.767894303351698, w0=-2.111387180706263, w1=1.013771942270566\n",
      "Gradient Descent(444/999): loss=4.766846415756184, w0=-2.11460372132913, w1=1.014095078665365\n",
      "Gradient Descent(445/999): loss=4.765802302591315, w0=-2.117814463829311, w1=1.0144176325758494\n",
      "Gradient Descent(446/999): loss=4.764761950261813, w0=-2.121019418658478, w1=1.0147396050520026\n",
      "Gradient Descent(447/999): loss=4.763725345221363, w0=-2.1242185962494626, w1=1.0150609971419156\n",
      "Gradient Descent(448/999): loss=4.762692473972447, w0=-2.1274120070162903, w1=1.0153818098917904\n",
      "Gradient Descent(449/999): loss=4.7616633230661645, w0=-2.1305996613542137, w1=1.0157020443459428\n",
      "Gradient Descent(450/999): loss=4.760637879102053, w0=-2.1337815696397477, w1=1.0160217015468063\n",
      "Gradient Descent(451/999): loss=4.7596161287279255, w0=-2.1369577422307025, w1=1.0163407825349353\n",
      "Gradient Descent(452/999): loss=4.75859805863968, w0=-2.140128189466217, w1=1.0166592883490084\n",
      "Gradient Descent(453/999): loss=4.757583655581141, w0=-2.1432929216667933, w1=1.016977220025832\n",
      "Gradient Descent(454/999): loss=4.7565729063438775, w0=-2.1464519491343292, w1=1.0172945786003436\n",
      "Gradient Descent(455/999): loss=4.755565797767038, w0=-2.149605282152153, w1=1.017611365105615\n",
      "Gradient Descent(456/999): loss=4.7545623167371724, w0=-2.1527529309850553, w1=1.0179275805728554\n",
      "Gradient Descent(457/999): loss=4.753562450188067, w0=-2.155894905879325, w1=1.018243226031416\n",
      "Gradient Descent(458/999): loss=4.752566185100569, w0=-2.159031217062779, w1=1.0185583025087923\n",
      "Gradient Descent(459/999): loss=4.751573508502425, w0=-2.1621618747448, w1=1.0188728110306269\n",
      "Gradient Descent(460/999): loss=4.750584407468099, w0=-2.165286889116365, w1=1.0191867526207141\n",
      "Gradient Descent(461/999): loss=4.7495988691186195, w0=-2.1684062703500824, w1=1.0195001283010032\n",
      "Gradient Descent(462/999): loss=4.7486168806214, w0=-2.1715200286002228, w1=1.0198129390916004\n",
      "Gradient Descent(463/999): loss=4.7476384291900775, w0=-2.174628174002753, w1=1.0201251860107736\n",
      "Gradient Descent(464/999): loss=4.7466635020843455, w0=-2.1777307166753683, w1=1.0204368700749549\n",
      "Gradient Descent(465/999): loss=4.745692086609786, w0=-2.180827666717527, w1=1.0207479922987448\n",
      "Gradient Descent(466/999): loss=4.744724170117706, w0=-2.1839190342104806, w1=1.021058553694914\n",
      "Gradient Descent(467/999): loss=4.743759740004974, w0=-2.1870048292173094, w1=1.0213685552744083\n",
      "Gradient Descent(468/999): loss=4.742798783713851, w0=-2.1900850617829537, w1=1.0216779980463508\n",
      "Gradient Descent(469/999): loss=4.741841288731832, w0=-2.1931597419342466, w1=1.0219868830180456\n",
      "Gradient Descent(470/999): loss=4.740887242591484, w0=-2.1962288796799467, w1=1.0222952111949812\n",
      "Gradient Descent(471/999): loss=4.739936632870274, w0=-2.1992924850107713, w1=1.0226029835808335\n",
      "Gradient Descent(472/999): loss=4.738989447190421, w0=-2.2023505678994284, w1=1.0229102011774691\n",
      "Gradient Descent(473/999): loss=4.7380456732187275, w0=-2.205403138300649, w1=1.023216864984949\n",
      "Gradient Descent(474/999): loss=4.737105298666415, w0=-2.2084502061512206, w1=1.0235229760015305\n",
      "Gradient Descent(475/999): loss=4.736168311288973, w0=-2.2114917813700172, w1=1.0238285352236731\n",
      "Gradient Descent(476/999): loss=4.73523469888599, w0=-2.2145278738580343, w1=1.0241335436460386\n",
      "Gradient Descent(477/999): loss=4.734304449301005, w0=-2.2175584934984194, w1=1.0244380022614963\n",
      "Gradient Descent(478/999): loss=4.733377550421341, w0=-2.220583650156505, w1=1.024741912061126\n",
      "Gradient Descent(479/999): loss=4.7324539901779525, w0=-2.22360335367984, w1=1.025045274034221\n",
      "Gradient Descent(480/999): loss=4.731533756545262, w0=-2.226617613898222, w1=1.0253480891682911\n",
      "Gradient Descent(481/999): loss=4.73061683754101, w0=-2.22962644062373, w1=1.0256503584490657\n",
      "Gradient Descent(482/999): loss=4.729703221226097, w0=-2.2326298436507557, w1=1.025952082860498\n",
      "Gradient Descent(483/999): loss=4.7287928957044265, w0=-2.2356278327560353, w1=1.026253263384767\n",
      "Gradient Descent(484/999): loss=4.727885849122752, w0=-2.238620417698681, w1=1.0265539010022815\n",
      "Gradient Descent(485/999): loss=4.726982069670519, w0=-2.2416076082202143, w1=1.0268539966916828\n",
      "Gradient Descent(486/999): loss=4.726081545579717, w0=-2.2445894140445963, w1=1.0271535514298484\n",
      "Gradient Descent(487/999): loss=4.725184265124721, w0=-2.247565844878259, w1=1.027452566191895\n",
      "Gradient Descent(488/999): loss=4.724290216622143, w0=-2.2505369104101387, w1=1.0277510419511808\n",
      "Gradient Descent(489/999): loss=4.7233993884306775, w0=-2.2535026203117057, w1=1.0280489796793102\n",
      "Gradient Descent(490/999): loss=4.722511768950947, w0=-2.256462984236997, w1=1.0283463803461357\n",
      "Gradient Descent(491/999): loss=4.721627346625359, w0=-2.2594180118226475, w1=1.0286432449197622\n",
      "Gradient Descent(492/999): loss=4.7207461099379495, w0=-2.26236771268792, w1=1.0289395743665486\n",
      "Gradient Descent(493/999): loss=4.7198680474142325, w0=-2.2653120964347386, w1=1.0292353696511127\n",
      "Gradient Descent(494/999): loss=4.718993147621053, w0=-2.268251172647719, w1=1.0295306317363329\n",
      "Gradient Descent(495/999): loss=4.71812139916644, w0=-2.2711849508941993, w1=1.029825361583352\n",
      "Gradient Descent(496/999): loss=4.717252790699452, w0=-2.2741134407242716, w1=1.0301195601515802\n",
      "Gradient Descent(497/999): loss=4.716387310910036, w0=-2.2770366516708136, w1=1.0304132283986989\n",
      "Gradient Descent(498/999): loss=4.715524948528875, w0=-2.2799545932495184, w1=1.0307063672806624\n",
      "Gradient Descent(499/999): loss=4.7146656923272445, w0=-2.2828672749589267, w1=1.0309989777517024\n",
      "Gradient Descent(500/999): loss=4.7138095311168655, w0=-2.2857747062804568, w1=1.0312910607643297\n",
      "Gradient Descent(501/999): loss=4.712956453749759, w0=-2.288676896678436, w1=1.0315826172693392\n",
      "Gradient Descent(502/999): loss=4.712106449118099, w0=-2.2915738556001313, w1=1.0318736482158113\n",
      "Gradient Descent(503/999): loss=4.711259506154067, w0=-2.2944655924757797, w1=1.032164154551115\n",
      "Gradient Descent(504/999): loss=4.710415613829716, w0=-2.2973521167186197, w1=1.0324541372209128\n",
      "Gradient Descent(505/999): loss=4.7095747611568175, w0=-2.3002334377249216, w1=1.0327435971691619\n",
      "Gradient Descent(506/999): loss=4.708736937186721, w0=-2.303109564874018, w1=1.0330325353381178\n",
      "Gradient Descent(507/999): loss=4.707902131010217, w0=-2.3059805075283335, w1=1.0333209526683376\n",
      "Gradient Descent(508/999): loss=4.707070331757382, w0=-2.3088462750334173, w1=1.033608850098683\n",
      "Gradient Descent(509/999): loss=4.706241528597455, w0=-2.3117068767179716, w1=1.0338962285663238\n",
      "Gradient Descent(510/999): loss=4.70541571073868, w0=-2.3145623218938827, w1=1.0341830890067396\n",
      "Gradient Descent(511/999): loss=4.704592867428176, w0=-2.3174126198562517, w1=1.0344694323537242\n",
      "Gradient Descent(512/999): loss=4.70377298795179, w0=-2.3202577798834243, w1=1.034755259539388\n",
      "Gradient Descent(513/999): loss=4.702956061633962, w0=-2.3230978112370213, w1=1.0350405714941615\n",
      "Gradient Descent(514/999): loss=4.702142077837591, w0=-2.325932723161968, w1=1.0353253691467974\n",
      "Gradient Descent(515/999): loss=4.701331025963876, w0=-2.3287625248865247, w1=1.0356096534243748\n",
      "Gradient Descent(516/999): loss=4.700522895452208, w0=-2.3315872256223176, w1=1.0358934252523015\n",
      "Gradient Descent(517/999): loss=4.699717675780004, w0=-2.334406834564368, w1=1.0361766855543169\n",
      "Gradient Descent(518/999): loss=4.698915356462592, w0=-2.3372213608911214, w1=1.0364594352524954\n",
      "Gradient Descent(519/999): loss=4.6981159270530615, w0=-2.3400308137644794, w1=1.0367416752672496\n",
      "Gradient Descent(520/999): loss=4.697319377142128, w0=-2.3428352023298276, w1=1.0370234065173325\n",
      "Gradient Descent(521/999): loss=4.696525696358007, w0=-2.3456345357160666, w1=1.0373046299198414\n",
      "Gradient Descent(522/999): loss=4.6957348743662655, w0=-2.348428823035641, w1=1.03758534639022\n",
      "Gradient Descent(523/999): loss=4.694946900869703, w0=-2.35121807338457, w1=1.0378655568422621\n",
      "Gradient Descent(524/999): loss=4.694161765608201, w0=-2.354002295842475, w1=1.0381452621881142\n",
      "Gradient Descent(525/999): loss=4.6933794583586, w0=-2.356781499472612, w1=1.0384244633382789\n",
      "Gradient Descent(526/999): loss=4.692599968934568, w0=-2.359555693321899, w1=1.0387031612016169\n",
      "Gradient Descent(527/999): loss=4.691823287186458, w0=-2.3623248864209456, w1=1.038981356685351\n",
      "Gradient Descent(528/999): loss=4.691049403001182, w0=-2.3650890877840833, w1=1.0392590506950687\n",
      "Gradient Descent(529/999): loss=4.69027830630208, w0=-2.3678483064093947, w1=1.039536244134725\n",
      "Gradient Descent(530/999): loss=4.689509987048791, w0=-2.370602551278742, w1=1.039812937906645\n",
      "Gradient Descent(531/999): loss=4.688744435237114, w0=-2.3733518313577973, w1=1.040089132911528\n",
      "Gradient Descent(532/999): loss=4.687981640898881, w0=-2.3760961555960702, w1=1.0403648300484494\n",
      "Gradient Descent(533/999): loss=4.6872215941018345, w0=-2.378835532926939, w1=1.0406400302148637\n",
      "Gradient Descent(534/999): loss=4.686464284949491, w0=-2.381569972267678, w1=1.0409147343066083\n",
      "Gradient Descent(535/999): loss=4.685709703581009, w0=-2.3842994825194883, w1=1.041188943217905\n",
      "Gradient Descent(536/999): loss=4.684957840171069, w0=-2.3870240725675242, w1=1.0414626578413637\n",
      "Gradient Descent(537/999): loss=4.684208684929745, w0=-2.3897437512809248, w1=1.0417358790679863\n",
      "Gradient Descent(538/999): loss=4.683462228102366, w0=-2.392458527512841, w1=1.0420086077871673\n",
      "Gradient Descent(539/999): loss=4.682718459969402, w0=-2.395168410100466, w1=1.0422808448866987\n",
      "Gradient Descent(540/999): loss=4.681977370846331, w0=-2.3978734078650623, w1=1.042552591252772\n",
      "Gradient Descent(541/999): loss=4.681238951083514, w0=-2.4005735296119917, w1=1.0428238477699816\n",
      "Gradient Descent(542/999): loss=4.680503191066068, w0=-2.4032687841307427, w1=1.0430946153213267\n",
      "Gradient Descent(543/999): loss=4.6797700812137455, w0=-2.405959180194961, w1=1.0433648947882153\n",
      "Gradient Descent(544/999): loss=4.679039611980803, w0=-2.408644726562476, w1=1.043634687050466\n",
      "Gradient Descent(545/999): loss=4.678311773855885, w0=-2.4113254319753312, w1=1.0439039929863125\n",
      "Gradient Descent(546/999): loss=4.677586557361888, w0=-2.414001305159811, w1=1.0441728134724042\n",
      "Gradient Descent(547/999): loss=4.676863953055852, w0=-2.41667235482647, w1=1.044441149383811\n",
      "Gradient Descent(548/999): loss=4.676143951528825, w0=-2.419338589670162, w1=1.0447090015940252\n",
      "Gradient Descent(549/999): loss=4.675426543405748, w0=-2.4220000183700656, w1=1.0449763709749644\n",
      "Gradient Descent(550/999): loss=4.674711719345328, w0=-2.4246566495897164, w1=1.045243258396975\n",
      "Gradient Descent(551/999): loss=4.6739994700399246, w0=-2.4273084919770316, w1=1.045509664728834\n",
      "Gradient Descent(552/999): loss=4.673289786215413, w0=-2.4299555541643407, w1=1.0457755908377522\n",
      "Gradient Descent(553/999): loss=4.672582658631084, w0=-2.4325978447684125, w1=1.0460410375893778\n",
      "Gradient Descent(554/999): loss=4.671878078079505, w0=-2.4352353723904825, w1=1.046306005847798\n",
      "Gradient Descent(555/999): loss=4.671176035386413, w0=-2.4378681456162825, w1=1.046570496475543\n",
      "Gradient Descent(556/999): loss=4.6704765214105874, w0=-2.4404961730160672, w1=1.0468345103335874\n",
      "Gradient Descent(557/999): loss=4.669779527043733, w0=-2.443119463144643, w1=1.0470980482813548\n",
      "Gradient Descent(558/999): loss=4.669085043210366, w0=-2.4457380245413947, w1=1.0473611111767187\n",
      "Gradient Descent(559/999): loss=4.66839306086769, w0=-2.4483518657303147, w1=1.0476236998760067\n",
      "Gradient Descent(560/999): loss=4.667703571005476, w0=-2.45096099522003, w1=1.0478858152340025\n",
      "Gradient Descent(561/999): loss=4.667016564645957, w0=-2.4535654215038303, w1=1.0481474581039494\n",
      "Gradient Descent(562/999): loss=4.666332032843698, w0=-2.456165153059694, w1=1.0484086293375523\n",
      "Gradient Descent(563/999): loss=4.665649966685488, w0=-2.458760198350319, w1=1.0486693297849807\n",
      "Gradient Descent(564/999): loss=4.664970357290219, w0=-2.461350565823147, w1=1.048929560294872\n",
      "Gradient Descent(565/999): loss=4.664293195808771, w0=-2.4639362639103926, w1=1.0491893217143338\n",
      "Gradient Descent(566/999): loss=4.6636184734239015, w0=-2.466517301029071, w1=1.0494486148889461\n",
      "Gradient Descent(567/999): loss=4.662946181350124, w0=-2.469093685581025, w1=1.0497074406627656\n",
      "Gradient Descent(568/999): loss=4.662276310833599, w0=-2.471665425952951, w1=1.0499657998783265\n",
      "Gradient Descent(569/999): loss=4.661608853152018, w0=-2.474232530516429, w1=1.0502236933766451\n",
      "Gradient Descent(570/999): loss=4.660943799614486, w0=-2.4767950076279486, w1=1.0504811219972212\n",
      "Gradient Descent(571/999): loss=4.660281141561417, w0=-2.4793528656289343, w1=1.0507380865780418\n",
      "Gradient Descent(572/999): loss=4.6596208703644155, w0=-2.4819061128457762, w1=1.0509945879555826\n",
      "Gradient Descent(573/999): loss=4.65896297742616, w0=-2.4844547575898543, w1=1.0512506269648123\n",
      "Gradient Descent(574/999): loss=4.658307454180302, w0=-2.4869988081575665, w1=1.0515062044391936\n",
      "Gradient Descent(575/999): loss=4.657654292091346, w0=-2.489538272830356, w1=1.0517613212106878\n",
      "Gradient Descent(576/999): loss=4.657003482654543, w0=-2.4920731598747383, w1=1.0520159781097556\n",
      "Gradient Descent(577/999): loss=4.656355017395771, w0=-2.4946034775423267, w1=1.0522701759653612\n",
      "Gradient Descent(578/999): loss=4.655708887871439, w0=-2.497129234069861, w1=1.0525239156049746\n",
      "Gradient Descent(579/999): loss=4.655065085668368, w0=-2.4996504376792332, w1=1.0527771978545741\n",
      "Gradient Descent(580/999): loss=4.6544236024036785, w0=-2.5021670965775145, w1=1.0530300235386487\n",
      "Gradient Descent(581/999): loss=4.653784429724687, w0=-2.504679218956982, w1=1.0532823934802016\n",
      "Gradient Descent(582/999): loss=4.653147559308802, w0=-2.507186812995146, w1=1.0535343085007525\n",
      "Gradient Descent(583/999): loss=4.6525129828634, w0=-2.509689886854775, w1=1.05378576942034\n",
      "Gradient Descent(584/999): loss=4.651880692125733, w0=-2.512188448683924, w1=1.0540367770575247\n",
      "Gradient Descent(585/999): loss=4.651250678862817, w0=-2.514682506615961, w1=1.0542873322293915\n",
      "Gradient Descent(586/999): loss=4.650622934871314, w0=-2.5171720687695913, w1=1.0545374357515525\n",
      "Gradient Descent(587/999): loss=4.649997451977443, w0=-2.5196571432488866, w1=1.0547870884381496\n",
      "Gradient Descent(588/999): loss=4.649374222036858, w0=-2.52213773814331, w1=1.055036291101857\n",
      "Gradient Descent(589/999): loss=4.648753236934556, w0=-2.524613861527742, w1=1.0552850445538842\n",
      "Gradient Descent(590/999): loss=4.648134488584754, w0=-2.5270855214625088, w1=1.055533349603978\n",
      "Gradient Descent(591/999): loss=4.6475179689308, w0=-2.529552725993405, w1=1.0557812070604262\n",
      "Gradient Descent(592/999): loss=4.646903669945062, w0=-2.532015483151724, w1=1.0560286177300593\n",
      "Gradient Descent(593/999): loss=4.64629158362882, w0=-2.53447380095428, w1=1.056275582418253\n",
      "Gradient Descent(594/999): loss=4.6456817020121655, w0=-2.5369276874034377, w1=1.056522101928932\n",
      "Gradient Descent(595/999): loss=4.645074017153899, w0=-2.5393771504871365, w1=1.056768177064571\n",
      "Gradient Descent(596/999): loss=4.644468521141421, w0=-2.541822198178916, w1=1.0570138086261986\n",
      "Gradient Descent(597/999): loss=4.6438652060906405, w0=-2.5442628384379438, w1=1.0572589974133997\n",
      "Gradient Descent(598/999): loss=4.643264064145856, w0=-2.546699079209039, w1=1.0575037442243178\n",
      "Gradient Descent(599/999): loss=4.642665087479663, w0=-2.549130928422701, w1=1.057748049855657\n",
      "Gradient Descent(600/999): loss=4.642068268292856, w0=-2.5515583939951316, w1=1.0579919151026862\n",
      "Gradient Descent(601/999): loss=4.6414735988143185, w0=-2.5539814838282653, w1=1.0582353407592402\n",
      "Gradient Descent(602/999): loss=4.6408810713009245, w0=-2.556400205809791, w1=1.0584783276177228\n",
      "Gradient Descent(603/999): loss=4.64029067803744, w0=-2.55881456781318, w1=1.0587208764691094\n",
      "Gradient Descent(604/999): loss=4.63970241133642, w0=-2.5612245776977107, w1=1.0589629881029503\n",
      "Gradient Descent(605/999): loss=4.6391162635381065, w0=-2.563630243308494, w1=1.0592046633073717\n",
      "Gradient Descent(606/999): loss=4.63853222701034, w0=-2.5660315724765, w1=1.0594459028690795\n",
      "Gradient Descent(607/999): loss=4.637950294148439, w0=-2.5684285730185823, w1=1.0596867075733616\n",
      "Gradient Descent(608/999): loss=4.637370457375125, w0=-2.570821252737504, w1=1.0599270782040897\n",
      "Gradient Descent(609/999): loss=4.636792709140407, w0=-2.573209619421962, w1=1.0601670155437235\n",
      "Gradient Descent(610/999): loss=4.636217041921488, w0=-2.575593680846615, w1=1.0604065203733115\n",
      "Gradient Descent(611/999): loss=4.635643448222671, w0=-2.5779734447721063, w1=1.0606455934724943\n",
      "Gradient Descent(612/999): loss=4.635071920575255, w0=-2.58034891894509, w1=1.0608842356195078\n",
      "Gradient Descent(613/999): loss=4.634502451537443, w0=-2.582720111098256, w1=1.0611224475911842\n",
      "Gradient Descent(614/999): loss=4.633935033694242, w0=-2.585087028950355, w1=1.0613602301629559\n",
      "Gradient Descent(615/999): loss=4.633369659657368, w0=-2.5874496802062246, w1=1.0615975841088572\n",
      "Gradient Descent(616/999): loss=4.6328063220651465, w0=-2.589808072556813, w1=1.0618345102015276\n",
      "Gradient Descent(617/999): loss=4.6322450135824225, w0=-2.5921622136792055, w1=1.0620710092122134\n",
      "Gradient Descent(618/999): loss=4.63168572690046, w0=-2.594512111236648, w1=1.0623070819107707\n",
      "Gradient Descent(619/999): loss=4.631128454736851, w0=-2.5968577728785727, w1=1.0625427290656682\n",
      "Gradient Descent(620/999): loss=4.630573189835415, w0=-2.5991992062406233, w1=1.0627779514439888\n",
      "Gradient Descent(621/999): loss=4.630019924966111, w0=-2.6015364189446797, w1=1.0630127498114332\n",
      "Gradient Descent(622/999): loss=4.6294686529249365, w0=-2.603869418598882, w1=1.0632471249323217\n",
      "Gradient Descent(623/999): loss=4.628919366533843, w0=-2.606198212797657, w1=1.0634810775695966\n",
      "Gradient Descent(624/999): loss=4.62837205864063, w0=-2.6085228091217405, w1=1.063714608484825\n",
      "Gradient Descent(625/999): loss=4.627826722118864, w0=-2.6108432151382037, w1=1.0639477184382018\n",
      "Gradient Descent(626/999): loss=4.627283349867778, w0=-2.613159438400478, w1=1.0641804081885504\n",
      "Gradient Descent(627/999): loss=4.626741934812184, w0=-2.615471486448379, w1=1.0644126784933277\n",
      "Gradient Descent(628/999): loss=4.626202469902374, w0=-2.6177793668081297, w1=1.064644530108624\n",
      "Gradient Descent(629/999): loss=4.625664948114038, w0=-2.620083086992388, w1=1.0648759637891674\n",
      "Gradient Descent(630/999): loss=4.625129362448163, w0=-2.6223826545002686, w1=1.0651069802883253\n",
      "Gradient Descent(631/999): loss=4.62459570593095, w0=-2.6246780768173688, w1=1.0653375803581073\n",
      "Gradient Descent(632/999): loss=4.6240639716137135, w0=-2.626969361415792, w1=1.065567764749167\n",
      "Gradient Descent(633/999): loss=4.623534152572803, w0=-2.6292565157541725, w1=1.0657975342108053\n",
      "Gradient Descent(634/999): loss=4.623006241909502, w0=-2.6315395472777, w1=1.0660268894909721\n",
      "Gradient Descent(635/999): loss=4.622480232749945, w0=-2.6338184634181436, w1=1.0662558313362696\n",
      "Gradient Descent(636/999): loss=4.621956118245025, w0=-2.636093271593875, w1=1.0664843604919532\n",
      "Gradient Descent(637/999): loss=4.621433891570306, w0=-2.638363979209895, w1=1.0667124777019357\n",
      "Gradient Descent(638/999): loss=4.620913545925933, w0=-2.6406305936578547, w1=1.0669401837087886\n",
      "Gradient Descent(639/999): loss=4.620395074536542, w0=-2.642893122316082, w1=1.0671674792537453\n",
      "Gradient Descent(640/999): loss=4.619878470651173, w0=-2.6451515725496044, w1=1.0673943650767024\n",
      "Gradient Descent(641/999): loss=4.619363727543187, w0=-2.6474059517101733, w1=1.067620841916223\n",
      "Gradient Descent(642/999): loss=4.618850838510172, w0=-2.6496562671362875, w1=1.0678469105095392\n",
      "Gradient Descent(643/999): loss=4.618339796873855, w0=-2.651902526153218, w1=1.0680725715925536\n",
      "Gradient Descent(644/999): loss=4.6178305959800205, w0=-2.654144736073031, w1=1.0682978258998428\n",
      "Gradient Descent(645/999): loss=4.617323229198418, w0=-2.6563829041946123, w1=1.068522674164659\n",
      "Gradient Descent(646/999): loss=4.6168176899226845, w0=-2.65861703780369, w1=1.0687471171189327\n",
      "Gradient Descent(647/999): loss=4.61631397157025, w0=-2.66084714417286, w1=1.0689711554932748\n",
      "Gradient Descent(648/999): loss=4.615812067582249, w0=-2.6630732305616074, w1=1.0691947900169796\n",
      "Gradient Descent(649/999): loss=4.61531197142345, w0=-2.665295304216333, w1=1.0694180214180262\n",
      "Gradient Descent(650/999): loss=4.6148136765821555, w0=-2.6675133723703737, w1=1.0696408504230823\n",
      "Gradient Descent(651/999): loss=4.614317176570122, w0=-2.6697274422440285, w1=1.069863277757505\n",
      "Gradient Descent(652/999): loss=4.613822464922478, w0=-2.671937521044581, w1=1.070085304145344\n",
      "Gradient Descent(653/999): loss=4.613329535197642, w0=-2.674143615966323, w1=1.0703069303093442\n",
      "Gradient Descent(654/999): loss=4.6128383809772275, w0=-2.676345734190578, w1=1.0705281569709473\n",
      "Gradient Descent(655/999): loss=4.61234899586597, w0=-2.6785438828857235, w1=1.0707489848502947\n",
      "Gradient Descent(656/999): loss=4.611861373491643, w0=-2.6807380692072167, w1=1.0709694146662294\n",
      "Gradient Descent(657/999): loss=4.611375507504967, w0=-2.6829283002976156, w1=1.071189447136299\n",
      "Gradient Descent(658/999): loss=4.6108913915795355, w0=-2.6851145832866035, w1=1.0714090829767575\n",
      "Gradient Descent(659/999): loss=4.610409019411731, w0=-2.687296925291011, w1=1.0716283229025678\n",
      "Gradient Descent(660/999): loss=4.609928384720636, w0=-2.6894753334148405, w1=1.071847167627404\n",
      "Gradient Descent(661/999): loss=4.609449481247962, w0=-2.691649814749289, w1=1.0720656178636536\n",
      "Gradient Descent(662/999): loss=4.60897230275796, w0=-2.6938203763727704, w1=1.0722836743224204\n",
      "Gradient Descent(663/999): loss=4.608496843037341, w0=-2.6959870253509397, w1=1.0725013377135257\n",
      "Gradient Descent(664/999): loss=4.608023095895198, w0=-2.6981497687367146, w1=1.0727186087455116\n",
      "Gradient Descent(665/999): loss=4.60755105516292, w0=-2.7003086135702996, w1=1.0729354881256434\n",
      "Gradient Descent(666/999): loss=4.60708071469412, w0=-2.7024635668792087, w1=1.0731519765599107\n",
      "Gradient Descent(667/999): loss=4.606612068364545, w0=-2.7046146356782885, w1=1.073368074753031\n",
      "Gradient Descent(668/999): loss=4.606145110072004, w0=-2.7067618269697395, w1=1.0735837834084514\n",
      "Gradient Descent(669/999): loss=4.605679833736286, w0=-2.708905147743141, w1=1.0737991032283511\n",
      "Gradient Descent(670/999): loss=4.605216233299077, w0=-2.7110446049754726, w1=1.0740140349136431\n",
      "Gradient Descent(671/999): loss=4.60475430272389, w0=-2.7131802056311374, w1=1.0742285791639772\n",
      "Gradient Descent(672/999): loss=4.604294035995977, w0=-2.715311956661984, w1=1.0744427366777423\n",
      "Gradient Descent(673/999): loss=4.603835427122256, w0=-2.717439865007331, w1=1.074656508152068\n",
      "Gradient Descent(674/999): loss=4.603378470131233, w0=-2.719563937593986, w1=1.0748698942828272\n",
      "Gradient Descent(675/999): loss=4.602923159072921, w0=-2.7216841813362724, w1=1.075082895764639\n",
      "Gradient Descent(676/999): loss=4.602469488018765, w0=-2.723800603136049, w1=1.0752955132908697\n",
      "Gradient Descent(677/999): loss=4.602017451061565, w0=-2.725913209882733, w1=1.0755077475536359\n",
      "Gradient Descent(678/999): loss=4.6015670423153985, w0=-2.728022008453323, w1=1.0757195992438071\n",
      "Gradient Descent(679/999): loss=4.601118255915542, w0=-2.730127005712422, w1=1.0759310690510067\n",
      "Gradient Descent(680/999): loss=4.6006710860184, w0=-2.732228208512258, w1=1.0761421576636157\n",
      "Gradient Descent(681/999): loss=4.600225526801421, w0=-2.734325623692707, w1=1.0763528657687735\n",
      "Gradient Descent(682/999): loss=4.59978157246303, w0=-2.7364192580813165, w1=1.0765631940523814\n",
      "Gradient Descent(683/999): loss=4.5993392172225445, w0=-2.7385091184933255, w1=1.0767731431991043\n",
      "Gradient Descent(684/999): loss=4.598898455320108, w0=-2.740595211731689, w1=1.0769827138923727\n",
      "Gradient Descent(685/999): loss=4.598459281016607, w0=-2.742677544587098, w1=1.077191906814385\n",
      "Gradient Descent(686/999): loss=4.598021688593604, w0=-2.7447561238380036, w1=1.0774007226461102\n",
      "Gradient Descent(687/999): loss=4.597585672353254, w0=-2.746830956250637, w1=1.0776091620672898\n",
      "Gradient Descent(688/999): loss=4.597151226618241, w0=-2.7489020485790334, w1=1.0778172257564398\n",
      "Gradient Descent(689/999): loss=4.596718345731694, w0=-2.7509694075650533, w1=1.0780249143908531\n",
      "Gradient Descent(690/999): loss=4.596287024057118, w0=-2.7530330399384035, w1=1.078232228646602\n",
      "Gradient Descent(691/999): loss=4.595857255978325, w0=-2.755092952416661, w1=1.0784391691985398\n",
      "Gradient Descent(692/999): loss=4.595429035899349, w0=-2.757149151705293, w1=1.0786457367203033\n",
      "Gradient Descent(693/999): loss=4.595002358244386, w0=-2.7592016444976792, w1=1.0788519318843153\n",
      "Gradient Descent(694/999): loss=4.5945772174577115, w0=-2.7612504374751348, w1=1.0790577553617862\n",
      "Gradient Descent(695/999): loss=4.594153608003618, w0=-2.7632955373069303, w1=1.0792632078227167\n",
      "Gradient Descent(696/999): loss=4.59373152436633, w0=-2.7653369506503154, w1=1.0794682899358996\n",
      "Gradient Descent(697/999): loss=4.593310961049945, w0=-2.767374684150538, w1=1.0796730023689225\n",
      "Gradient Descent(698/999): loss=4.592891912578358, w0=-2.769408744440868, w1=1.079877345788169\n",
      "Gradient Descent(699/999): loss=4.592474373495182, w0=-2.771439138142618, w1=1.0800813208588216\n",
      "Gradient Descent(700/999): loss=4.592058338363686, w0=-2.773465871865166, w1=1.0802849282448643\n",
      "Gradient Descent(701/999): loss=4.591643801766725, w0=-2.775488952205975, w1=1.080488168609084\n",
      "Gradient Descent(702/999): loss=4.591230758306666, w0=-2.7775083857506155, w1=1.080691042613072\n",
      "Gradient Descent(703/999): loss=4.590819202605313, w0=-2.7795241790727867, w1=1.0808935509172284\n",
      "Gradient Descent(704/999): loss=4.590409129303844, w0=-2.781536338734339, w1=1.081095694180762\n",
      "Gradient Descent(705/999): loss=4.590000533062744, w0=-2.7835448712852933, w1=1.0812974730616933\n",
      "Gradient Descent(706/999): loss=4.589593408561724, w0=-2.7855497832638645, w1=1.0814988882168572\n",
      "Gradient Descent(707/999): loss=4.589187750499661, w0=-2.787551081196481, w1=1.0816999403019039\n",
      "Gradient Descent(708/999): loss=4.588783553594528, w0=-2.7895487715978073, w1=1.0819006299713023\n",
      "Gradient Descent(709/999): loss=4.5883808125833205, w0=-2.7915428609707638, w1=1.0821009578783414\n",
      "Gradient Descent(710/999): loss=4.587979522221995, w0=-2.793533355806549, w1=1.082300924675132\n",
      "Gradient Descent(711/999): loss=4.587579677285394, w0=-2.7955202625846614, w1=1.0825005310126106\n",
      "Gradient Descent(712/999): loss=4.587181272567177, w0=-2.7975035877729177, w1=1.0826997775405394\n",
      "Gradient Descent(713/999): loss=4.586784302879765, w0=-2.7994833378274775, w1=1.0828986649075094\n",
      "Gradient Descent(714/999): loss=4.586388763054259, w0=-2.801459519192862, w1=1.0830971937609428\n",
      "Gradient Descent(715/999): loss=4.585994647940378, w0=-2.8034321383019747, w1=1.0832953647470946\n",
      "Gradient Descent(716/999): loss=4.585601952406391, w0=-2.8054012015761245, w1=1.0834931785110549\n",
      "Gradient Descent(717/999): loss=4.585210671339057, w0=-2.8073667154250446, w1=1.0836906356967508\n",
      "Gradient Descent(718/999): loss=4.5848207996435475, w0=-2.809328686246914, w1=1.0838877369469488\n",
      "Gradient Descent(719/999): loss=4.584432332243385, w0=-2.811287120428378, w1=1.084084482903257\n",
      "Gradient Descent(720/999): loss=4.584045264080381, w0=-2.81324202434457, w1=1.0842808742061265\n",
      "Gradient Descent(721/999): loss=4.5836595901145625, w0=-2.8151934043591322, w1=1.084476911494854\n",
      "Gradient Descent(722/999): loss=4.583275305324111, w0=-2.817141266824234, w1=1.0846725954075842\n",
      "Gradient Descent(723/999): loss=4.582892404705298, w0=-2.819085618080596, w1=1.0848679265813108\n",
      "Gradient Descent(724/999): loss=4.582510883272418, w0=-2.821026464457508, w1=1.0850629056518801\n",
      "Gradient Descent(725/999): loss=4.582130736057722, w0=-2.822963812272851, w1=1.0852575332539913\n",
      "Gradient Descent(726/999): loss=4.581751958111355, w0=-2.8248976678331177, w1=1.0854518100212005\n",
      "Gradient Descent(727/999): loss=4.5813745445012914, w0=-2.8268280374334327, w1=1.0856457365859207\n",
      "Gradient Descent(728/999): loss=4.580998490313272, w0=-2.8287549273575725, w1=1.0858393135794258\n",
      "Gradient Descent(729/999): loss=4.580623790650736, w0=-2.830678343877987, w1=1.0860325416318513\n",
      "Gradient Descent(730/999): loss=4.580250440634761, w0=-2.8325982932558187, w1=1.086225421372197\n",
      "Gradient Descent(731/999): loss=4.579878435403997, w0=-2.8345147817409253, w1=1.0864179534283285\n",
      "Gradient Descent(732/999): loss=4.579507770114606, w0=-2.8364278155718967, w1=1.0866101384269804\n",
      "Gradient Descent(733/999): loss=4.579138439940198, w0=-2.8383374009760787, w1=1.0868019769937567\n",
      "Gradient Descent(734/999): loss=4.5787704400717635, w0=-2.8402435441695904, w1=1.0869934697531343\n",
      "Gradient Descent(735/999): loss=4.578403765717618, w0=-2.842146251357347, w1=1.0871846173284638\n",
      "Gradient Descent(736/999): loss=4.578038412103337, w0=-2.8440455287330777, w1=1.087375420341973\n",
      "Gradient Descent(737/999): loss=4.57767437447169, w0=-2.845941382479347, w1=1.0875658794147671\n",
      "Gradient Descent(738/999): loss=4.577311648082583, w0=-2.847833818767576, w1=1.0877559951668327\n",
      "Gradient Descent(739/999): loss=4.576950228212997, w0=-2.8497228437580597, w1=1.087945768217038\n",
      "Gradient Descent(740/999): loss=4.576590110156924, w0=-2.851608463599989, w1=1.0881351991831358\n",
      "Gradient Descent(741/999): loss=4.576231289225308, w0=-2.853490684431471, w1=1.088324288681766\n",
      "Gradient Descent(742/999): loss=4.575873760745979, w0=-2.855369512379547, w1=1.088513037328456\n",
      "Gradient Descent(743/999): loss=4.575517520063601, w0=-2.857244953560215, w1=1.088701445737624\n",
      "Gradient Descent(744/999): loss=4.575162562539603, w0=-2.8591170140784476, w1=1.0888895145225812\n",
      "Gradient Descent(745/999): loss=4.574808883552121, w0=-2.8609857000282126, w1=1.0890772442955325\n",
      "Gradient Descent(746/999): loss=4.574456478495943, w0=-2.8628510174924933, w1=1.0892646356675795\n",
      "Gradient Descent(747/999): loss=4.574105342782437, w0=-2.8647129725433076, w1=1.0894516892487223\n",
      "Gradient Descent(748/999): loss=4.573755471839511, w0=-2.8665715712417277, w1=1.0896384056478612\n",
      "Gradient Descent(749/999): loss=4.573406861111529, w0=-2.8684268196379006, w1=1.0898247854727994\n",
      "Gradient Descent(750/999): loss=4.573059506059272, w0=-2.870278723771067, w1=1.090010829330244\n",
      "Gradient Descent(751/999): loss=4.572713402159867, w0=-2.8721272896695815, w1=1.0901965378258085\n",
      "Gradient Descent(752/999): loss=4.572368544906735, w0=-2.873972523350932, w1=1.090381911564015\n",
      "Gradient Descent(753/999): loss=4.572024929809526, w0=-2.8758144308217592, w1=1.0905669511482956\n",
      "Gradient Descent(754/999): loss=4.571682552394068, w0=-2.8776530180778765, w1=1.0907516571809948\n",
      "Gradient Descent(755/999): loss=4.5713414082023025, w0=-2.8794882911042885, w1=1.0909360302633713\n",
      "Gradient Descent(756/999): loss=4.571001492792229, w0=-2.8813202558752122, w1=1.0911200709956\n",
      "Gradient Descent(757/999): loss=4.570662801737846, w0=-2.8831489183540953, w1=1.0913037799767735\n",
      "Gradient Descent(758/999): loss=4.570325330629095, w0=-2.884974284493635, w1=1.0914871578049055\n",
      "Gradient Descent(759/999): loss=4.569989075071803, w0=-2.8867963602357993, w1=1.091670205076931\n",
      "Gradient Descent(760/999): loss=4.569654030687624, w0=-2.888615151511845, w1=1.0918529223887083\n",
      "Gradient Descent(761/999): loss=4.56932019311398, w0=-2.8904306642423365, w1=1.0920353103350233\n",
      "Gradient Descent(762/999): loss=4.568987558004012, w0=-2.8922429043371665, w1=1.0922173695095885\n",
      "Gradient Descent(763/999): loss=4.568656121026514, w0=-2.894051877695574, w1=1.0923991005050462\n",
      "Gradient Descent(764/999): loss=4.568325877865882, w0=-2.895857590206165, w1=1.0925805039129708\n",
      "Gradient Descent(765/999): loss=4.567996824222056, w0=-2.89766004774693, w1=1.0927615803238704\n",
      "Gradient Descent(766/999): loss=4.567668955810466, w0=-2.899459256185264, w1=1.0929423303271883\n",
      "Gradient Descent(767/999): loss=4.5673422683619735, w0=-2.901255221377985, w1=1.0931227545113058\n",
      "Gradient Descent(768/999): loss=4.567016757622818, w0=-2.903047949171355, w1=1.0933028534635427\n",
      "Gradient Descent(769/999): loss=4.56669241935456, w0=-2.9048374454010957, w1=1.0934826277701608\n",
      "Gradient Descent(770/999): loss=4.566369249334025, w0=-2.9066237158924104, w1=1.0936620780163653\n",
      "Gradient Descent(771/999): loss=4.566047243353257, w0=-2.908406766460002, w1=1.0938412047863058\n",
      "Gradient Descent(772/999): loss=4.565726397219448, w0=-2.910186602908091, w1=1.0940200086630791\n",
      "Gradient Descent(773/999): loss=4.565406706754898, w0=-2.911963231030436, w1=1.0941984902287314\n",
      "Gradient Descent(774/999): loss=4.565088167796951, w0=-2.913736656610352, w1=1.0943766500642593\n",
      "Gradient Descent(775/999): loss=4.564770776197948, w0=-2.915506885420728, w1=1.0945544887496121\n",
      "Gradient Descent(776/999): loss=4.564454527825168, w0=-2.9172739232240477, w1=1.094732006863694\n",
      "Gradient Descent(777/999): loss=4.564139418560778, w0=-2.919037775772407, w1=1.0949092049843652\n",
      "Gradient Descent(778/999): loss=4.563825444301772, w0=-2.9207984488075334, w1=1.0950860836884444\n",
      "Gradient Descent(779/999): loss=4.563512600959928, w0=-2.9225559480608037, w1=1.0952626435517108\n",
      "Gradient Descent(780/999): loss=4.563200884461749, w0=-2.9243102792532643, w1=1.0954388851489054\n",
      "Gradient Descent(781/999): loss=4.562890290748406, w0=-2.926061448095648, w1=1.0956148090537334\n",
      "Gradient Descent(782/999): loss=4.562580815775695, w0=-2.927809460288394, w1=1.0957904158388658\n",
      "Gradient Descent(783/999): loss=4.562272455513976, w0=-2.9295543215216657, w1=1.0959657060759411\n",
      "Gradient Descent(784/999): loss=4.561965205948123, w0=-2.9312960374753696, w1=1.0961406803355673\n",
      "Gradient Descent(785/999): loss=4.561659063077476, w0=-2.9330346138191734, w1=1.0963153391873246\n",
      "Gradient Descent(786/999): loss=4.5613540229157845, w0=-2.934770056212525, w1=1.0964896831997653\n",
      "Gradient Descent(787/999): loss=4.5610500814911505, w0=-2.9365023703046704, w1=1.0966637129404178\n",
      "Gradient Descent(788/999): loss=4.560747234845985, w0=-2.938231561734672, w1=1.096837428975787\n",
      "Gradient Descent(789/999): loss=4.560445479036962, w0=-2.9399576361314277, w1=1.0970108318713567\n",
      "Gradient Descent(790/999): loss=4.560144810134948, w0=-2.9416805991136883, w1=1.0971839221915916\n",
      "Gradient Descent(791/999): loss=4.559845224224967, w0=-2.943400456290077, w1=1.0973567004999385\n",
      "Gradient Descent(792/999): loss=4.559546717406144, w0=-2.945117213259106, w1=1.0975291673588288\n",
      "Gradient Descent(793/999): loss=4.559249285791658, w0=-2.946830875609197, w1=1.0977013233296797\n",
      "Gradient Descent(794/999): loss=4.558952925508681, w0=-2.9485414489186965, w1=1.097873168972897\n",
      "Gradient Descent(795/999): loss=4.558657632698342, w0=-2.950248938755896, w1=1.0980447048478763\n",
      "Gradient Descent(796/999): loss=4.558363403515664, w0=-2.95195335067905, w1=1.0982159315130038\n",
      "Gradient Descent(797/999): loss=4.558070234129523, w0=-2.9536546902363936, w1=1.0983868495256603\n",
      "Gradient Descent(798/999): loss=4.557778120722592, w0=-2.9553529629661606, w1=1.0985574594422214\n",
      "Gradient Descent(799/999): loss=4.5574870594912955, w0=-2.9570481743966015, w1=1.09872776181806\n",
      "Gradient Descent(800/999): loss=4.557197046645756, w0=-2.958740330046002, w1=1.0988977572075473\n",
      "Gradient Descent(801/999): loss=4.5569080784097515, w0=-2.960429435422699, w1=1.0990674461640562\n",
      "Gradient Descent(802/999): loss=4.556620151020655, w0=-2.962115496025103, w1=1.0992368292399608\n",
      "Gradient Descent(803/999): loss=4.556333260729399, w0=-2.9637985173417105, w1=1.0994059069866409\n",
      "Gradient Descent(804/999): loss=4.556047403800417, w0=-2.9654785048511254, w1=1.099574679954481\n",
      "Gradient Descent(805/999): loss=4.555762576511595, w0=-2.967155464022076, w1=1.0997431486928748\n",
      "Gradient Descent(806/999): loss=4.555478775154231, w0=-2.9688294003134326, w1=1.0999113137502248\n",
      "Gradient Descent(807/999): loss=4.555195996032978, w0=-2.970500319174225, w1=1.100079175673945\n",
      "Gradient Descent(808/999): loss=4.554914235465801, w0=-2.9721682260436615, w1=1.1002467350104628\n",
      "Gradient Descent(809/999): loss=4.554633489783927, w0=-2.9738331263511446, w1=1.100413992305221\n",
      "Gradient Descent(810/999): loss=4.554353755331798, w0=-2.975495025516291, w1=1.1005809481026785\n",
      "Gradient Descent(811/999): loss=4.554075028467021, w0=-2.9771539289489466, w1=1.100747602946313\n",
      "Gradient Descent(812/999): loss=4.553797305560328, w0=-2.9788098420492064, w1=1.100913957378623\n",
      "Gradient Descent(813/999): loss=4.553520582995515, w0=-2.9804627702074313, w1=1.1010800119411284\n",
      "Gradient Descent(814/999): loss=4.553244857169412, w0=-2.9821127188042653, w1=1.1012457671743734\n",
      "Gradient Descent(815/999): loss=4.552970124491824, w0=-2.9837596932106534, w1=1.1014112236179274\n",
      "Gradient Descent(816/999): loss=4.552696381385485, w0=-2.9854036987878585, w1=1.1015763818103879\n",
      "Gradient Descent(817/999): loss=4.5524236242860185, w0=-2.98704474088748, w1=1.1017412422893806\n",
      "Gradient Descent(818/999): loss=4.5521518496418825, w0=-2.98868282485147, w1=1.101905805591563\n",
      "Gradient Descent(819/999): loss=4.551881053914331, w0=-2.990317956012152, w1=1.1020700722526242\n",
      "Gradient Descent(820/999): loss=4.551611233577363, w0=-2.991950139692236, w1=1.1022340428072888\n",
      "Gradient Descent(821/999): loss=4.551342385117678, w0=-2.993579381204839, w1=1.1023977177893167\n",
      "Gradient Descent(822/999): loss=4.551074505034629, w0=-2.9952056858534992, w1=1.102561097731506\n",
      "Gradient Descent(823/999): loss=4.55080758984018, w0=-2.9968290589321955, w1=1.1027241831656942\n",
      "Gradient Descent(824/999): loss=4.55054163605886, w0=-2.998449505725364, w1=1.10288697462276\n",
      "Gradient Descent(825/999): loss=4.550276640227712, w0=-3.0000670315079145, w1=1.1030494726326259\n",
      "Gradient Descent(826/999): loss=4.550012598896252, w0=-3.0016816415452485, w1=1.1032116777242582\n",
      "Gradient Descent(827/999): loss=4.549749508626433, w0=-3.003293341093276, w1=1.1033735904256698\n",
      "Gradient Descent(828/999): loss=4.549487365992583, w0=-3.0049021353984333, w1=1.1035352112639227\n",
      "Gradient Descent(829/999): loss=4.549226167581372, w0=-3.0065080296976987, w1=1.103696540765128\n",
      "Gradient Descent(830/999): loss=4.548965909991764, w0=-3.0081110292186106, w1=1.1038575794544485\n",
      "Gradient Descent(831/999): loss=4.548706589834974, w0=-3.009711139179285, w1=1.1040183278561004\n",
      "Gradient Descent(832/999): loss=4.548448203734425, w0=-3.0113083647884302, w1=1.1041787864933554\n",
      "Gradient Descent(833/999): loss=4.548190748325698, w0=-3.012902711245367, w1=1.1043389558885413\n",
      "Gradient Descent(834/999): loss=4.547934220256497, w0=-3.0144941837400427, w1=1.1044988365630448\n",
      "Gradient Descent(835/999): loss=4.547678616186597, w0=-3.01608278745305, w1=1.1046584290373125\n",
      "Gradient Descent(836/999): loss=4.547423932787805, w0=-3.017668527555642, w1=1.104817733830853\n",
      "Gradient Descent(837/999): loss=4.547170166743919, w0=-3.0192514092097515, w1=1.1049767514622382\n",
      "Gradient Descent(838/999): loss=4.546917314750677, w0=-3.0208314375680057, w1=1.1051354824491055\n",
      "Gradient Descent(839/999): loss=4.546665373515725, w0=-3.0224086177737437, w1=1.1052939273081595\n",
      "Gradient Descent(840/999): loss=4.54641433975856, w0=-3.0239829549610335, w1=1.1054520865551722\n",
      "Gradient Descent(841/999): loss=4.546164210210502, w0=-3.025554454254688, w1=1.1056099607049876\n",
      "Gradient Descent(842/999): loss=4.545914981614642, w0=-3.027123120770283, w1=1.10576755027152\n",
      "Gradient Descent(843/999): loss=4.545666650725801, w0=-3.028688959614172, w1=1.1059248557677583\n",
      "Gradient Descent(844/999): loss=4.54541921431049, w0=-3.030251975883504, w1=1.1060818777057666\n",
      "Gradient Descent(845/999): loss=4.545172669146869, w0=-3.03181217466624, w1=1.1062386165966853\n",
      "Gradient Descent(846/999): loss=4.544927012024698, w0=-3.03336956104117, w1=1.1063950729507344\n",
      "Gradient Descent(847/999): loss=4.544682239745306, w0=-3.0349241400779285, w1=1.1065512472772134\n",
      "Gradient Descent(848/999): loss=4.544438349121537, w0=-3.0364759168370115, w1=1.106707140084504\n",
      "Gradient Descent(849/999): loss=4.5441953369777215, w0=-3.0380248963697927, w1=1.1068627518800718\n",
      "Gradient Descent(850/999): loss=4.543953200149625, w0=-3.039571083718541, w1=1.1070180831704672\n",
      "Gradient Descent(851/999): loss=4.543711935484407, w0=-3.0411144839164357, w1=1.1071731344613274\n",
      "Gradient Descent(852/999): loss=4.543471539840592, w0=-3.0426551019875827, w1=1.1073279062573786\n",
      "Gradient Descent(853/999): loss=4.543232010088012, w0=-3.0441929429470327, w1=1.1074823990624372\n",
      "Gradient Descent(854/999): loss=4.542993343107778, w0=-3.045728011800795, w1=1.1076366133794109\n",
      "Gradient Descent(855/999): loss=4.542755535792234, w0=-3.0472603135458565, w1=1.107790549710301\n",
      "Gradient Descent(856/999): loss=4.542518585044917, w0=-3.048789853170195, w1=1.1079442085562048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(857/999): loss=4.542282487780517, w0=-3.0503166356527984, w1=1.1080975904173147\n",
      "Gradient Descent(858/999): loss=4.5420472409248385, w0=-3.0518406659636788, w1=1.1082506957929228\n",
      "Gradient Descent(859/999): loss=4.541812841414758, w0=-3.053361949063889, w1=1.1084035251814206\n",
      "Gradient Descent(860/999): loss=4.541579286198186, w0=-3.05488048990554, w1=1.1085560790803013\n",
      "Gradient Descent(861/999): loss=4.541346572234025, w0=-3.056396293431815, w1=1.1087083579861612\n",
      "Gradient Descent(862/999): loss=4.541114696492135, w0=-3.0579093645769877, w1=1.108860362394702\n",
      "Gradient Descent(863/999): loss=4.5408836559532855, w0=-3.0594197082664367, w1=1.109012092800731\n",
      "Gradient Descent(864/999): loss=4.540653447609125, w0=-3.0609273294166623, w1=1.109163549698164\n",
      "Gradient Descent(865/999): loss=4.540424068462133, w0=-3.0624322329353024, w1=1.1093147335800269\n",
      "Gradient Descent(866/999): loss=4.540195515525591, w0=-3.0639344237211485, w1=1.109465644938456\n",
      "Gradient Descent(867/999): loss=4.539967785823538, w0=-3.065433906664161, w1=1.1096162842647008\n",
      "Gradient Descent(868/999): loss=4.53974087639073, w0=-3.066930686645487, w1=1.1097666520491256\n",
      "Gradient Descent(869/999): loss=4.539514784272603, w0=-3.0684247685374726, w1=1.1099167487812107\n",
      "Gradient Descent(870/999): loss=4.539289506525238, w0=-3.069916157203683, w1=1.1100665749495537\n",
      "Gradient Descent(871/999): loss=4.53906504021532, w0=-3.071404857498916, w1=1.1102161310418717\n",
      "Gradient Descent(872/999): loss=4.538841382420095, w0=-3.0728908742692176, w1=1.1103654175450026\n",
      "Gradient Descent(873/999): loss=4.5386185302273425, w0=-3.0743742123518984, w1=1.110514434944907\n",
      "Gradient Descent(874/999): loss=4.538396480735327, w0=-3.07585487657555, w1=1.1106631837266687\n",
      "Gradient Descent(875/999): loss=4.538175231052767, w0=-3.0773328717600594, w1=1.1108116643744983\n",
      "Gradient Descent(876/999): loss=4.537954778298796, w0=-3.0788082027166253, w1=1.1109598773717324\n",
      "Gradient Descent(877/999): loss=4.537735119602923, w0=-3.080280874247774, w1=1.1111078232008371\n",
      "Gradient Descent(878/999): loss=4.537516252104995, w0=-3.081750891147374, w1=1.1112555023434083\n",
      "Gradient Descent(879/999): loss=4.537298172955165, w0=-3.0832182582006538, w1=1.1114029152801743\n",
      "Gradient Descent(880/999): loss=4.537080879313848, w0=-3.084682980184215, w1=1.1115500624909966\n",
      "Gradient Descent(881/999): loss=4.5368643683516865, w0=-3.0861450618660493, w1=1.1116969444548714\n",
      "Gradient Descent(882/999): loss=4.536648637249516, w0=-3.0876045080055534, w1=1.111843561649932\n",
      "Gradient Descent(883/999): loss=4.536433683198326, w0=-3.0890613233535453, w1=1.1119899145534495\n",
      "Gradient Descent(884/999): loss=4.536219503399223, w0=-3.0905155126522783, w1=1.1121360036418346\n",
      "Gradient Descent(885/999): loss=4.536006095063397, w0=-3.091967080635458, w1=1.1122818293906394\n",
      "Gradient Descent(886/999): loss=4.5357934554120805, w0=-3.0934160320282573, w1=1.112427392274559\n",
      "Gradient Descent(887/999): loss=4.535581581676518, w0=-3.0948623715473302, w1=1.112572692767432\n",
      "Gradient Descent(888/999): loss=4.535370471097922, w0=-3.09630610390083, w1=1.1127177313422438\n",
      "Gradient Descent(889/999): loss=4.53516012092745, w0=-3.097747233788422, w1=1.1128625084711268\n",
      "Gradient Descent(890/999): loss=4.53495052842615, w0=-3.099185765901301, w1=1.113007024625362\n",
      "Gradient Descent(891/999): loss=4.534741690864946, w0=-3.1006217049222045, w1=1.1131512802753818\n",
      "Gradient Descent(892/999): loss=4.534533605524584, w0=-3.102055055525429, w1=1.1132952758907695\n",
      "Gradient Descent(893/999): loss=4.534326269695611, w0=-3.103485822376846, w1=1.1134390119402626\n",
      "Gradient Descent(894/999): loss=4.534119680678329, w0=-3.104914010133915, w1=1.1135824888917536\n",
      "Gradient Descent(895/999): loss=4.533913835782764, w0=-3.106339623445701, w1=1.1137257072122915\n",
      "Gradient Descent(896/999): loss=4.533708732328635, w0=-3.1077626669528886, w1=1.113868667368083\n",
      "Gradient Descent(897/999): loss=4.5335043676453095, w0=-3.1091831452877967, w1=1.1140113698244951\n",
      "Gradient Descent(898/999): loss=4.533300739071781, w0=-3.110601063074394, w1=1.1141538150460557\n",
      "Gradient Descent(899/999): loss=4.533097843956623, w0=-3.1120164249283144, w1=1.114296003496455\n",
      "Gradient Descent(900/999): loss=4.532895679657962, w0=-3.113429235456871, w1=1.114437935638548\n",
      "Gradient Descent(901/999): loss=4.532694243543437, w0=-3.1148394992590727, w1=1.1145796119343547\n",
      "Gradient Descent(902/999): loss=4.532493532990172, w0=-3.1162472209256373, w1=1.1147210328450625\n",
      "Gradient Descent(903/999): loss=4.532293545384737, w0=-3.1176524050390086, w1=1.1148621988310277\n",
      "Gradient Descent(904/999): loss=4.532094278123115, w0=-3.119055056173369, w1=1.1150031103517763\n",
      "Gradient Descent(905/999): loss=4.53189572861067, w0=-3.1204551788946557, w1=1.1151437678660066\n",
      "Gradient Descent(906/999): loss=4.531697894262111, w0=-3.1218527777605756, w1=1.1152841718315896\n",
      "Gradient Descent(907/999): loss=4.531500772501459, w0=-3.12324785732062, w1=1.1154243227055707\n",
      "Gradient Descent(908/999): loss=4.531304360762013, w0=-3.124640422116079, w1=1.115564220944172\n",
      "Gradient Descent(909/999): loss=4.531108656486318, w0=-3.126030476680057, w1=1.1157038670027932\n",
      "Gradient Descent(910/999): loss=4.53091365712613, w0=-3.1274180255374864, w1=1.1158432613360127\n",
      "Gradient Descent(911/999): loss=4.530719360142383, w0=-3.1288030732051437, w1=1.1159824043975894\n",
      "Gradient Descent(912/999): loss=4.53052576300516, w0=-3.130185624191663, w1=1.116121296640465\n",
      "Gradient Descent(913/999): loss=4.530332863193653, w0=-3.131565682997551, w1=1.1162599385167638\n",
      "Gradient Descent(914/999): loss=4.530140658196133, w0=-3.1329432541152027, w1=1.1163983304777958\n",
      "Gradient Descent(915/999): loss=4.529949145509925, w0=-3.134318342028914, w1=1.116536472974057\n",
      "Gradient Descent(916/999): loss=4.52975832264136, w0=-3.1356909512148983, w1=1.1166743664552319\n",
      "Gradient Descent(917/999): loss=4.529568187105754, w0=-3.1370610861412995, w1=1.1168120113701938\n",
      "Gradient Descent(918/999): loss=4.529378736427375, w0=-3.138428751268208, w1=1.116949408167007\n",
      "Gradient Descent(919/999): loss=4.5291899681394066, w0=-3.139793951047673, w1=1.1170865572929285\n",
      "Gradient Descent(920/999): loss=4.529001879783919, w0=-3.141156689923721, w1=1.1172234591944088\n",
      "Gradient Descent(921/999): loss=4.5288144689118335, w0=-3.142516972332365, w1=1.1173601143170935\n",
      "Gradient Descent(922/999): loss=4.528627733082892, w0=-3.1438748027016237, w1=1.1174965231058251\n",
      "Gradient Descent(923/999): loss=4.528441669865631, w0=-3.1452301854515325, w1=1.1176326860046442\n",
      "Gradient Descent(924/999): loss=4.528256276837341, w0=-3.14658312499416, w1=1.1177686034567906\n",
      "Gradient Descent(925/999): loss=4.52807155158404, w0=-3.147933625733622, w1=1.117904275904706\n",
      "Gradient Descent(926/999): loss=4.527887491700442, w0=-3.149281692066094, w1=1.1180397037900336\n",
      "Gradient Descent(927/999): loss=4.527704094789922, w0=-3.1506273283798283, w1=1.118174887553621\n",
      "Gradient Descent(928/999): loss=4.52752135846449, w0=-3.1519705390551667, w1=1.1183098276355212\n",
      "Gradient Descent(929/999): loss=4.527339280344758, w0=-3.153311328464554, w1=1.1184445244749934\n",
      "Gradient Descent(930/999): loss=4.527157858059903, w0=-3.154649700972555, w1=1.1185789785105058\n",
      "Gradient Descent(931/999): loss=4.52697708924765, w0=-3.155985660935866, w1=1.1187131901797356\n",
      "Gradient Descent(932/999): loss=4.526796971554227, w0=-3.1573192127033294, w1=1.1188471599195713\n",
      "Gradient Descent(933/999): loss=4.52661750263434, w0=-3.1586503606159493, w1=1.1189808881661139\n",
      "Gradient Descent(934/999): loss=4.526438680151146, w0=-3.1599791090069043, w1=1.119114375354678\n",
      "Gradient Descent(935/999): loss=4.526260501776218, w0=-3.1613054622015624, w1=1.1192476219197935\n",
      "Gradient Descent(936/999): loss=4.526082965189513, w0=-3.162629424517494, w1=1.119380628295208\n",
      "Gradient Descent(937/999): loss=4.525906068079348, w0=-3.1639510002644875, w1=1.1195133949138856\n",
      "Gradient Descent(938/999): loss=4.525729808142364, w0=-3.1652701937445618, w1=1.1196459222080113\n",
      "Gradient Descent(939/999): loss=4.525554183083503, w0=-3.1665870092519817, w1=1.1197782106089904\n",
      "Gradient Descent(940/999): loss=4.525379190615968, w0=-3.1679014510732704, w1=1.1199102605474507\n",
      "Gradient Descent(941/999): loss=4.525204828461203, w0=-3.1692135234872247, w1=1.1200420724532436\n",
      "Gradient Descent(942/999): loss=4.525031094348856, w0=-3.1705232307649283, w1=1.1201736467554462\n",
      "Gradient Descent(943/999): loss=4.524857986016757, w0=-3.171830577169766, w1=1.1203049838823613\n",
      "Gradient Descent(944/999): loss=4.524685501210879, w0=-3.1731355669574373, w1=1.1204360842615202\n",
      "Gradient Descent(945/999): loss=4.524513637685319, w0=-3.1744382043759707, w1=1.1205669483196834\n",
      "Gradient Descent(946/999): loss=4.524342393202259, w0=-3.1757384936657367, w1=1.1206975764828424\n",
      "Gradient Descent(947/999): loss=4.524171765531945, w0=-3.1770364390594623, w1=1.1208279691762202\n",
      "Gradient Descent(948/999): loss=4.524001752452653, w0=-3.178332044782245, w1=1.1209581268242739\n",
      "Gradient Descent(949/999): loss=4.523832351750661, w0=-3.179625315051566, w1=1.121088049850695\n",
      "Gradient Descent(950/999): loss=4.523663561220221, w0=-3.1809162540773035, w1=1.1212177386784117\n",
      "Gradient Descent(951/999): loss=4.523495378663532, w0=-3.1822048660617477, w1=1.1213471937295894\n",
      "Gradient Descent(952/999): loss=4.523327801890707, w0=-3.1834911551996132, w1=1.1214764154256327\n",
      "Gradient Descent(953/999): loss=4.523160828719747, w0=-3.184775125678054, w1=1.1216054041871868\n",
      "Gradient Descent(954/999): loss=4.5229944569765115, w0=-3.1860567816766756, w1=1.1217341604341384\n",
      "Gradient Descent(955/999): loss=4.522828684494693, w0=-3.1873361273675496, w1=1.1218626845856172\n",
      "Gradient Descent(956/999): loss=4.522663509115789, w0=-3.1886131669152276, w1=1.1219909770599976\n",
      "Gradient Descent(957/999): loss=4.522498928689068, w0=-3.189887904476753, w1=1.1221190382748998\n",
      "Gradient Descent(958/999): loss=4.522334941071547, w0=-3.191160344201677, w1=1.1222468686471911\n",
      "Gradient Descent(959/999): loss=4.52217154412796, w0=-3.19243049023207, w1=1.1223744685929875\n",
      "Gradient Descent(960/999): loss=4.522008735730734, w0=-3.193698346702536, w1=1.1225018385276548\n",
      "Gradient Descent(961/999): loss=4.521846513759958, w0=-3.194963917740226, w1=1.1226289788658101\n",
      "Gradient Descent(962/999): loss=4.521684876103361, w0=-3.1962272074648523, w1=1.1227558900213233\n",
      "Gradient Descent(963/999): loss=4.521523820656275, w0=-3.1974882199887, w1=1.1228825724073177\n",
      "Gradient Descent(964/999): loss=4.521363345321613, w0=-3.1987469594166416, w1=1.1230090264361725\n",
      "Gradient Descent(965/999): loss=4.521203448009848, w0=-3.20000342984615, w1=1.1231352525195233\n",
      "Gradient Descent(966/999): loss=4.521044126638972, w0=-3.2012576353673126, w1=1.1232612510682636\n",
      "Gradient Descent(967/999): loss=4.520885379134478, w0=-3.2025095800628436, w1=1.1233870224925466\n",
      "Gradient Descent(968/999): loss=4.520727203429336, w0=-3.203759268008098, w1=1.1235125672017858\n",
      "Gradient Descent(969/999): loss=4.520569597463955, w0=-3.2050067032710845, w1=1.1236378856046567\n",
      "Gradient Descent(970/999): loss=4.520412559186166, w0=-3.2062518899124783, w1=1.123762978109098\n",
      "Gradient Descent(971/999): loss=4.520256086551191, w0=-3.207494831985636, w1=1.1238878451223138\n",
      "Gradient Descent(972/999): loss=4.520100177521616, w0=-3.2087355335366063, w1=1.1240124870507733\n",
      "Gradient Descent(973/999): loss=4.51994483006737, w0=-3.2099739986041453, w1=1.1241369043002134\n",
      "Gradient Descent(974/999): loss=4.519790042165688, w0=-3.2112102312197286, w1=1.1242610972756395\n",
      "Gradient Descent(975/999): loss=4.519635811801097, w0=-3.212444235407565, w1=1.124385066381327\n",
      "Gradient Descent(976/999): loss=4.519482136965377, w0=-3.213676015184609, w1=1.1245088120208229\n",
      "Gradient Descent(977/999): loss=4.519329015657548, w0=-3.214905574560574, w1=1.124632334596946\n",
      "Gradient Descent(978/999): loss=4.519176445883832, w0=-3.216132917537946, w1=1.1247556345117895\n",
      "Gradient Descent(979/999): loss=4.519024425657637, w0=-3.2173580481119957, w1=1.1248787121667216\n",
      "Gradient Descent(980/999): loss=4.518872952999525, w0=-3.218580970270792, w1=1.1250015679623875\n",
      "Gradient Descent(981/999): loss=4.518722025937183, w0=-3.219801687995215, w1=1.1251242022987094\n",
      "Gradient Descent(982/999): loss=4.518571642505412, w0=-3.221020205258969, w1=1.125246615574889\n",
      "Gradient Descent(983/999): loss=4.518421800746084, w0=-3.2222365260285954, w1=1.1253688081894087\n",
      "Gradient Descent(984/999): loss=4.518272498708126, w0=-3.223450654263485, w1=1.1254907805400318\n",
      "Gradient Descent(985/999): loss=4.518123734447491, w0=-3.2246625939158915, w1=1.1256125330238056\n",
      "Gradient Descent(986/999): loss=4.517975506027139, w0=-3.2258723489309453, w1=1.1257340660370607\n",
      "Gradient Descent(987/999): loss=4.5178278115170025, w0=-3.227079923246664, w1=1.125855379975414\n",
      "Gradient Descent(988/999): loss=4.517680648993968, w0=-3.228285320793967, w1=1.125976475233769\n",
      "Gradient Descent(989/999): loss=4.51753401654185, w0=-3.2294885454966886, w1=1.1260973522063173\n",
      "Gradient Descent(990/999): loss=4.517387912251361, w0=-3.230689601271589, w1=1.12621801128654\n",
      "Gradient Descent(991/999): loss=4.517242334220096, w0=-3.2318884920283684, w1=1.126338452867209\n",
      "Gradient Descent(992/999): loss=4.517097280552498, w0=-3.2330852216696795, w1=1.126458677340388\n",
      "Gradient Descent(993/999): loss=4.51695274935984, w0=-3.23427979409114, w1=1.1265786850974338\n",
      "Gradient Descent(994/999): loss=4.516808738760196, w0=-3.235472213181345, w1=1.1266984765289982\n",
      "Gradient Descent(995/999): loss=4.516665246878423, w0=-3.236662482821881, w1=1.1268180520250288\n",
      "Gradient Descent(996/999): loss=4.516522271846125, w0=-3.2378506068873367, w1=1.12693741197477\n",
      "Gradient Descent(997/999): loss=4.516379811801644, w0=-3.2390365892453166, w1=1.1270565567667645\n",
      "Gradient Descent(998/999): loss=4.516237864890023, w0=-3.2402204337564537, w1=1.1271754867888548\n",
      "Gradient Descent(999/999): loss=4.516096429262984, w0=-3.241402144274422, w1=1.1272942024281842\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-3.24140214,  1.1272942 ])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g, cost = gradientDescent(X, y, theta, alpha, iters)\n",
    "g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can compute the cost (error) of the trained model using our fitted parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.031911006157827"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "computeCost(X, y, g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plot the linear model along with the data to visually see how well it fits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,'Predicted Profit vs. Population Size')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.linspace(data.Population.min(), data.Population.max(), 100)\n",
    "f = g[0] + (g[1] * x)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "ax.plot(x, f, 'r', label='Prediction')\n",
    "ax.scatter(data.Population, data.Profit, label='Traning Data')\n",
    "ax.legend(loc=2)\n",
    "ax.set_xlabel('Population')\n",
    "ax.set_ylabel('Profit')\n",
    "ax.set_title('Predicted Profit vs. Population Size')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks pretty good!  Since the gradient decent function also outputs a vector with the cost at each training iteration, we can plot that as well.  Notice that the cost always decreases - this is an example of a convex optimization problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,'Error vs. Training Epoch')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "ax.plot(np.arange(iters), cost, 'r')\n",
    "ax.set_xlabel('Iterations')\n",
    "ax.set_ylabel('Cost')\n",
    "ax.set_title('Error vs. Training Epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
